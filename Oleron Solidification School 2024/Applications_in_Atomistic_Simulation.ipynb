{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Oi_ZLYw4-xsi",
        "ntnZb9nkouIl",
        "q3SmqfZllipv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "So far, we've delved into the foundational concepts and practical implementations of clustering, classification, and regression. In this notebook, we will explore how these machine learning techniques can be applied to solve complex problems in the field of material science.\n",
        "\n",
        "Understanding the behavior of materials at an atomic level is crucial for the development of new materials with enhanced properties. Traditionally, these insights are obtained through experiments and simulations, with classical molecular dynamics (MD) being a prominent simulation technique. In classical molecular dynamics, we simulate the interactions between atoms over time, providing detailed information about the system's evolution. However, as we will see bellow, analyzing and interpreting this data can be challenging due to its complexity and high dimensionality. This is where machine learning comes into play, offering powerful tools to extract meaningful patterns and make predictions.\n",
        "\n",
        "Throughout this notebook, we will import data generated from classical molecular dynamics simulations and demonstrate how to preprocess, analyze, and visualize this data using machine learning techniques."
      ],
      "metadata": {
        "id": "67KnsUYqzDxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collectiong all the data and packages we will need:\n",
        "!rm -r /content/Notebooks\n",
        "!git clone https://github.com/jpalastus/Notebooks.git\n",
        "!pip install ase dscribe"
      ],
      "metadata": {
        "id": "4NL6KQrKB3yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------\n",
        "# Clustering Globals Structure Descriptors from Copper Melting MD (Easy case!)\n",
        "\n",
        "Here, we provided you some data that come from a LAMMPS molecular dynamicsof a 1000 Cu atoms FCC supercell from 1000-2000K, at constant pressure of 1atm, using the Morse potential. To make the things easy here, we already have a descriptor of those structures generated for you (with the script provided in the github). Our job here is to try to interpret this data and separate the liquid and solid structures one from the other. To do so, we will use some clustering in the [MBTR descriptor](https://singroup.github.io/dscribe/latest/tutorials/descriptors/mbtr.html) with $k=2$ (equivalent to well known $g(r)$)."
      ],
      "metadata": {
        "id": "Oi_ZLYw4-xsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE, locally_linear_embedding\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "X = np.load('/content/Notebooks/Oleron Solidification School 2024/data/MD_melting/mbtr_copper.npy')\n",
        "X=X[1:] #We are removing the initial frame (perfect FCC)."
      ],
      "metadata": {
        "id": "HH0ehoID-45u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import cm,colormaps\n",
        "\n",
        "N=len(X)\n",
        "print(N)\n",
        "cmap = colormaps['jet']\n",
        "colours=[cmap(float(i)/float(N-1)) for i in range(N)]\n",
        "\n",
        "xmin=1.5; xmax=5.0\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.xlabel(\"Bound Length ($\\AA$)\")\n",
        "plt.ylabel(\"Cluster-averaged MBTR for k=2 (Radial Distribution)\")\n",
        "for i in range(1,len(X)):\n",
        "  xx=X[i]#+0.001*i    <--------- Try adding this!\n",
        "  N=len(xx)\n",
        "  xx_plot=(np.array(range(N))/float(N))*(xmax-xmin)+xmin\n",
        "  plt.plot(xx_plot, xx,'-', linewidth=1, c = colours[i])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ShxNoTYtwa47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib\n",
        "cm_preference='bwr'\n",
        "\n",
        "# We are using dbscan!  eps=0.2 and min_sample=10 are `fine tunned` to this aplication.\n",
        "# You can play with then to see what happens.\n",
        "model = DBSCAN(eps=0.20,\n",
        "               min_samples=10,\n",
        "               metric='l1') # L1 metric here acts as an integral of the absolut difference\n",
        "\n",
        "model.fit(X)\n",
        "\n",
        "y_pred = model.labels_\n",
        "nb_c=max(y_pred)+1\n",
        "\n",
        "print(nb_c)\n",
        "\n",
        "cmap = matplotlib.cm.get_cmap(cm_preference)\n",
        "colours=[cmap(float(i)/float(nb_c-1)) for i in range(nb_c)]\n",
        "\n",
        "xmin=1.5; xmax=5.0\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.xlabel(\"Bound Length ($\\AA$)\")\n",
        "plt.ylabel(\"Cluster-averaged MBTR for k=2 (Radial Distribution)\")\n",
        "for i in range(nb_c):\n",
        "  aux=X[y_pred==i]\n",
        "  xx=np.mean(np.array(aux), axis=0)\n",
        "  N=len(xx)\n",
        "  xx_plot=(np.array(range(N))/float(N))*(xmax-xmin)+xmin\n",
        "  plt.plot(xx_plot, xx,'-', linewidth=5, c = colours[i])\n",
        "  std=np.std(np.array(aux), axis=0)\n",
        "  plt.fill_between(xx_plot, xx-std, xx+std, alpha=.3, linewidth=0, color = colours[i])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qRDh1-H3EJkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE, Isomap, MDS\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "\n",
        "#We are just including some technics for high dimensional data visualization, so you can try to see how the data is structured.\n",
        "#Check https://scikit-learn.org/stable/modules/manifold.html\n",
        "\n",
        "fig, axs = plt.subplots(2, 2,figsize=(10, 10))\n",
        "fig.suptitle('Manifold Learning Examples')\n",
        "\n",
        "# PCA\n",
        "pca=PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "axs[0,0].scatter(X_pca[y_pred!=-1][:, 0], X_pca[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[0,0].scatter(X_pca[y_pred==-1][:, 0], X_pca[y_pred==-1][:,1], marker='x', c = 'k')\n",
        "axs[0,0].set_xlabel('PCA 1 ('+\"{:.2f}\".format(pca.explained_variance_ratio_[0]*100.0)+'%)')\n",
        "axs[0,0].set_ylabel('PCA 2 ('+\"{:.2f}\".format(pca.explained_variance_ratio_[1]*100.0)+'%)')\n",
        "axs[0,0].set_title(\"PCA Projection\")\n",
        "\n",
        "\n",
        "# TSNE\n",
        "X_tsne = TSNE(n_components=2, perplexity=10,early_exaggeration=50,metric=\"l1\").fit_transform(X)\n",
        "axs[0,1].scatter(X_tsne[y_pred!=-1][:, 0], X_tsne[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[0,1].scatter(X_tsne[y_pred==-1][:, 0], X_tsne[y_pred==-1][:,1], marker='x', c = 'k')\n",
        "axs[0,1].set_title(\"t-SNE Projection\")\n",
        "\n",
        "# Isomap\n",
        "X_isomap = Isomap(n_components=2,n_neighbors=10,metric=\"l1\").fit_transform(X)\n",
        "axs[1,0].scatter(X_isomap[y_pred!=-1][:, 0], X_isomap[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[1,0].scatter(X_isomap[y_pred==-1][:, 0], X_isomap[y_pred==-1][:,1], marker='x', c = 'k')\n",
        "axs[1,0].set_title(\"Isomap Projection\")\n",
        "\n",
        "# Multidimensional scaling\n",
        "M=manhattan_distances(X)\n",
        "X_mds = MDS(n_components=2,dissimilarity='precomputed').fit_transform(M)\n",
        "axs[1,1].scatter(X_mds[y_pred!=-1][:, 0], X_mds[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[1,1].scatter(X_mds[y_pred==-1][:, 0], X_mds[y_pred==-1][:,1], marker='x', c = 'k')\n",
        "axs[1,1].set_title(\"Multidimensional scaling projection\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "-4nfN-JZzE-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xmin=1.5; xmax=5.0\n",
        "direct0=pca.components_[0]\n",
        "direct1=pca.components_[1]\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.xlabel(\"Bound Length ($\\AA$)\")\n",
        "plt.ylabel(\"PCA component direction\")\n",
        "xx_plot=(np.array(range(N))/float(N))*(xmax-xmin)+xmin\n",
        "plt.plot(xx_plot, direct0,'-', linewidth=5, c = 'green',label='First component direction')\n",
        "plt.plot(xx_plot, direct1,'-', linewidth=5, c = 'magenta',label='Second component direction')\n",
        "for i in [10, 990]:\n",
        "  xx=X[i]\n",
        "  N=len(xx)\n",
        "  xx_plot=(np.array(range(N))/float(N))*(xmax-xmin)+xmin\n",
        "  plt.plot(xx_plot, xx,'--', linewidth=1, c ='k')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8lcB2y_U7G_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "df = pd.read_csv('/content/Notebooks/Oleron Solidification School 2024/data/MD_melting/lammps_thermostat_log.csv',delim_whitespace=True)\n",
        "temps = df['Temp'].to_numpy()[1:]\n",
        "cm_preference='jet'\n",
        "\n",
        "# Plot again using the temperatures as colors\n",
        "fig, axs = plt.subplots(2, 2,figsize=(12, 10))\n",
        "fig.suptitle('Manifold Learning Examples')\n",
        "\n",
        "# PCA\n",
        "axs[0,0].scatter(X_pca[:, 0], X_pca[:,1], marker='.', c = temps, cmap=cm_preference)\n",
        "axs[0,0].set_title(\"PCA Projection\")\n",
        "\n",
        "# TSNE\n",
        "axs[0,1].scatter(X_tsne[:, 0], X_tsne[:,1], marker='.', c = temps, cmap=cm_preference)\n",
        "axs[0,1].set_title(\"t-SNE Projection\")\n",
        "\n",
        "# Isomap\n",
        "axs[1,0].scatter(X_isomap[:, 0], X_isomap[:,1], marker='.', c = temps, cmap=cm_preference)\n",
        "axs[1,0].set_title(\"Isomap Projection\")\n",
        "\n",
        "# Multidimensional scaling\n",
        "aux=axs[1,1].scatter(X_mds[:, 0], X_mds[:,1], marker='.', c = temps, cmap=cm_preference)\n",
        "axs[1,1].set_title(\"Multidimensional scaling projection\")\n",
        "\n",
        "cax,kw = mpl.colorbar.make_axes([ax for ax in axs.flat])\n",
        "plt.colorbar(aux, cax=cax, label='Temperature (K)', **kw)\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "o5sZQe0NygF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from numpy import convolve\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "#N=5\n",
        "#plt.plot(convolve(temps[y_pred!=-1],np.ones(N)/N)[N:-N], convolve(y_pred[y_pred!=-1],np.ones(N)/N)[N:-N], c = (0.5,0.5,0.5))\n",
        "plt.scatter(temps[y_pred!=-1], y_pred[y_pred!=-1], marker='.', c = y_pred[y_pred!=-1], cmap='bwr')\n",
        "plt.scatter(temps[y_pred==-1], y_pred[y_pred==-1], marker='x', c = 'k')\n",
        "plt.ylim(-1.5, 1.5)\n",
        "plt.yticks([-1, 0, 1])\n",
        "plt.axvline(1358, color='k', linestyle='--')\n",
        "plt.text(1358, 0, 'Exp. Melting Point (1358K)', ha='center', va='center',rotation='vertical', backgroundcolor=(1, 1, 1, 0.8))\n",
        "plt.xlabel('Temperature')\n",
        "plt.ylabel('Cluster Label')\n",
        "plt.title('Temperature vs. Cluster Label')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PoD8FoU62g7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------\n",
        "# Clustering Local Descriptors from Al Molecular dynamics for Grain Growth\n",
        "\n",
        "This will be very similar to the exemple above, but now we are interested in data from the paper *Atom-centered symmetry functions for constructing high-dimensional neural network potentials* ([DOI:10.1063/1.3553717](https://doi.org/10.1063/1.3553717)) provided by Noel Jakse. We will again use a `dscribe` descriptor, now LMBTR (analogous to a $g(r)$ around a particular atom) to try to pinpoint the grain formation. The data we are using is from a particular frame, and includes randomly selected atoms (script provided in the github). We will visualize a classifier for the clustering istructure we discover on fine slices of the MD, to see the nucleation and grain formation."
      ],
      "metadata": {
        "id": "ntnZb9nkouIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE, locally_linear_embedding\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "X = np.load('/content/Notebooks/Oleron Solidification School 2024/data/Al_nucleation_MD/lmbtr_175ps.npy')\n"
      ],
      "metadata": {
        "id": "iywla0ICouIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import cm,colormaps\n",
        "\n",
        "N=len(X)\n",
        "print(N)\n",
        "cmap = colormaps['jet']\n",
        "colours=[cmap(float(i)/float(N-1)) for i in range(N)]\n",
        "\n",
        "xmin=2.0; xmax=4.5\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.xlabel(\"Bound Length ($\\AA$)\")\n",
        "plt.ylabel(\"Cluster-averaged MBTR for k=2 (Radial Distribution)\")\n",
        "for i in range(1,len(X)):\n",
        "  xx=X[i]#+0.001*i\n",
        "  N=len(xx)\n",
        "  xx_plot=(np.array(range(N))/float(N))*(xmax-xmin)+xmin\n",
        "  plt.plot(xx_plot, xx,'-', linewidth=1, c = colours[i])\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#How this data compare to the case above? Why they are so different?"
      ],
      "metadata": {
        "id": "7wAypelXouIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Lets have a look on how diferent our plots are between thenselves...\n",
        "distances = []\n",
        "distances_small = []\n",
        "for i in range(len(X)):\n",
        "  for j in range(i+1, len(X)):\n",
        "    aux=np.linalg.norm(X[i] - X[j], ord=1)\n",
        "    if aux<2:\n",
        "      distances_small.append(aux)\n",
        "    distances.append(aux)\n",
        "\n",
        "plt.hist(distances, bins=100)\n",
        "plt.xlabel('L1 Distance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of L1 Distances Between Elements of X')\n",
        "plt.show()\n",
        "\n",
        "#Lets zoom on the lowest distances...\n",
        "plt.hist(distances_small, bins=200)\n",
        "plt.xlabel('L1 Distance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of L1 Distances Between Elements of X')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q0sqNcFSsioL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib\n",
        "\n",
        "cm_preference='jet'\n",
        "nb_c = 5\n",
        "model = KMeans(n_clusters=nb_c\n",
        "               , init='random'\n",
        "               , n_init=100\n",
        "               , max_iter=1000\n",
        "               , tol=1e-04\n",
        "              )\n",
        "# For fun, you can check this one also! What happens? Can you explain?\n",
        "# OBS.: Remember to rerun this cell with only the kmeans before continuing!\n",
        "#model = DBSCAN(eps=1.5,\n",
        "#               min_samples=5,\n",
        "#               metric='l1')\n",
        "\n",
        "model.fit(X)\n",
        "\n",
        "y_pred = model.labels_\n",
        "nb_c=max(y_pred)+1\n",
        "\n",
        "print(nb_c)\n",
        "\n",
        "cmap = matplotlib.cm.get_cmap(cm_preference)\n",
        "colours=[cmap(float(i)/float(nb_c-1)) for i in range(nb_c)]\n",
        "\n",
        "xmin=2.0; xmax=4.5\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.xlabel(\"Bound Length ($\\AA$)\")\n",
        "plt.ylabel(\"Cluster-averaged MBTR for k=2 (Radial Distribution)\")\n",
        "for i in range(nb_c):\n",
        "  aux=X[y_pred==i]\n",
        "  xx=np.mean(np.array(aux), axis=0)\n",
        "  N=len(xx)\n",
        "  xx_plot=(np.array(range(N))/float(N))*(xmax-xmin)+xmin\n",
        "  plt.plot(xx_plot, xx,'-', linewidth=5, c = colours[i])\n",
        "  std=np.std(np.array(aux), axis=0)\n",
        "  plt.fill_between(xx_plot, xx-std, xx+std, alpha=.3, linewidth=0, color = colours[i])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SOPduzbGouIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vizualizing the data structure...\n",
        "\n",
        "from sklearn.manifold import TSNE, Isomap, MDS\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "\n",
        "X_pca = PCA(n_components=2).fit_transform(X)\n",
        "X_tsne = TSNE(n_components=2, perplexity=30,early_exaggeration=1.5,metric=\"l1\").fit_transform(X)\n",
        "X_isomap = Isomap(n_components=2,n_neighbors=30,metric=\"l1\").fit_transform(X)\n",
        "M=manhattan_distances(X)\n",
        "X_mds = MDS(n_components=2,dissimilarity='precomputed').fit_transform(M)"
      ],
      "metadata": {
        "id": "hyGsrDlAz3Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 2,figsize=(10, 10))\n",
        "fig.suptitle('Manifold Learning Examples')\n",
        "\n",
        "# PCA\n",
        "axs[0,0].scatter(X_pca[y_pred!=-1][:, 0], X_pca[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[0,0].scatter(X_pca[y_pred==-1][:, 0], X_pca[y_pred==-1][:,1], marker='x', c = 'k',alpha=0.1)\n",
        "axs[0,0].set_title(\"PCA Projection\")\n",
        "\n",
        "\n",
        "# TSNE\n",
        "axs[0,1].scatter(X_tsne[y_pred!=-1][:, 0], X_tsne[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[0,1].scatter(X_tsne[y_pred==-1][:, 0], X_tsne[y_pred==-1][:,1], marker='x', c = 'k',alpha=0.1)\n",
        "axs[0,1].set_title(\"t-SNE Projection\")\n",
        "\n",
        "# Isomap\n",
        "axs[1,0].scatter(X_isomap[y_pred!=-1][:, 0], X_isomap[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[1,0].scatter(X_isomap[y_pred==-1][:, 0], X_isomap[y_pred==-1][:,1], marker='x', c = 'k',alpha=0.1)\n",
        "axs[1,0].set_title(\"Isomap Projection\")\n",
        "\n",
        "# Multidimensional scaling\n",
        "axs[1,1].scatter(X_mds[y_pred!=-1][:, 0], X_mds[y_pred!=-1][:,1], marker='.', c = y_pred[y_pred!=-1], cmap=cm_preference)\n",
        "axs[1,1].scatter(X_mds[y_pred==-1][:, 0], X_mds[y_pred==-1][:,1], marker='x', c = 'k',alpha=0.1)\n",
        "axs[1,1].set_title(\"Multidimensional scaling projection\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "w2coC-8oouIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets now use this cluster labels to train a classifier!\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mlogreg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "Xm_train, Xm_test, ym_train, ym_test = train_test_split(X, y_pred, test_size=0.80, random_state=123)\n",
        "\n",
        "mlogreg.fit(Xm_train, ym_train)\n",
        "ym_pred = mlogreg.predict(X)\n",
        "proba_log = mlogreg.predict_proba(X)\n",
        "prob_pred=[proba_log[i,ym_pred[i]] for i in range(len(ym_pred))]"
      ],
      "metadata": {
        "id": "FLTzFeBf2Mss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you dont know what is a Confussion Matrix, check: https://fr.wikipedia.org/wiki/Matrice_de_confusion"
      ],
      "metadata": {
        "id": "YhXbLnPt6g2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_pred, ym_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Clustering Labels\")\n",
        "plt.ylabel(\"Model Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jGM9dAqq2RBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets now see how those classes are spacialy distributed in the molecular dynamics frames!\n",
        "gdrs=[]\n",
        "poss=[]\n",
        "times=[166,170,174,175,177]\n",
        "for t in times:\n",
        "  gdrs.append(np.load('/content/Notebooks/Oleron Solidification School 2024/data/Al_nucleation_MD/lmbtr_'+str(t)+'ps.pos.npy'))\n",
        "  poss.append(np.load('/content/Notebooks/Oleron Solidification School 2024/data/Al_nucleation_MD/slice_'+str(t)+'ps.pos.npy'))\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1,5,figsize=(22,4))\n",
        "fig.suptitle('Spacial distribution of labels')\n",
        "\n",
        "for i in range(5):\n",
        "  gdr=gdrs[i]\n",
        "  pos=poss[i]\n",
        "  colours= mlogreg.predict(gdr)\n",
        "  axs[i].scatter(pos[:,0],pos[:,1],marker='.', c = colours, cmap=cm_preference)\n",
        "  axs[i].set_title(\"t=\"+str(times[i])+\"ps\")\n",
        "  axs[i].set_xlabel('Position X ($\\t{\\AA}$ )')\n",
        "\n",
        "axs[0].set_ylabel('Position Y ($\\t{\\AA}$ )')\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Can you see when the grain apear? Can we count the grain size with it?\n",
        "# What you think the model learned about the pomains? And about the liquid?"
      ],
      "metadata": {
        "id": "J3ihLhIS31xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------\n",
        "# Checking the chalengy of organizing Local Minima structures with Cu$_{147}$ nanoclusters\n",
        "\n",
        "Here, we are checking some MBTR or Coulomb Matrixes that were computed for a local minima structures colection we generated with help of `dscribe` and [ABCluster](https://doi.org/10.1039/C5CP04060D). This is here more to showcase the complicated space we have where most of the structures are in a high energy configuration and only a few are realy closer to eachother and to the global minima configuration. This is didatical, because its very close to many real cenarious. We invite you to play with the data and try to propose analysis."
      ],
      "metadata": {
        "id": "q3SmqfZllipv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "y = np.load('/content/Notebooks/Oleron Solidification School 2024/data/Cu147_nanoclusters/energies.npy')\n",
        "#X = np.load('/content/Notebooks/Oleron Solidification School 2024/data/Cu147_nanoclusters/mbtr.npy')\n",
        "X = np.load('/content/Notebooks/Oleron Solidification School 2024/data/Cu147_nanoclusters/cm.npy')\n",
        "X=X[y<-454]\n",
        "y=y[y<-454]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_r = pca.fit(X).transform(X)\n",
        "\n",
        "# Percentage of variance explained for each components\n",
        "print(\n",
        "    \"explained variance ratio (first two components): %s\"\n",
        "    % str(pca.explained_variance_ratio_)\n",
        ")\n",
        "\n",
        "plt.figure()\n",
        "dist=plt.scatter(X_r[:,0], X_r[:,1], marker='.', c=y, cmap='jet')\n",
        "plt.title(\"PCA of the Chosen Representation\")\n",
        "cbar = plt.colorbar(dist)\n",
        "cbar.set_label('Energy', rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dSWz0jYGcS_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE, locally_linear_embedding\n",
        "import numpy as np\n",
        "\n",
        "##LOADING FILES\n",
        "#X = np.load('/content/Notebooks/Oleron Solidification School 2024/data/Cu147_nanoclusters/mbtr.npy')\n",
        "X = np.load('/content/Notebooks/Oleron Solidification School 2024/data/Cu147_nanoclusters/cm.npy')\n",
        "y = np.load('/content/Notebooks/Oleron Solidification School 2024/data/Cu147_nanoclusters/energies.npy')\n",
        "\n",
        "#Checking the energy distribution\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title(\"Energy Histogram\")\n",
        "ax.set_xlabel(\"Potential Energy (eV)\")\n",
        "ax.set_ylabel(\"Number of Structures\")\n",
        "N, bins, patches = ax.hist(y, bins=80, color=\"b\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Lets try to visualize some of the data....\n",
        "X=X[y<-453]\n",
        "y=y[y<-453]\n",
        "#X_r, _ = locally_linear_embedding(X,n_neighbors=50, n_components=2)\n",
        "X_r = TSNE(n_components=2,perplexity=5,early_exaggeration=10).fit_transform(X)\n",
        "\n",
        "plt.figure()\n",
        "dist=plt.scatter(X_r[:,0], X_r[:,1], marker='.', c=y, cmap='jet')\n",
        "plt.title(\"PCA of the Chosen Representation\")\n",
        "cbar = plt.colorbar(dist)\n",
        "cbar.set_label('Energy', rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SbqXXsDzgv7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "dist=plt.scatter(X_r[:,0], X_r[:,1], marker='o', c=y, cmap='jet')\n",
        "plt.title(\"PCA of the Chosen Representation\")\n",
        "cbar = plt.colorbar(dist)\n",
        "cbar.set_label('Potential Energy (eV)', rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GygsvzXYTv6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------\n",
        "# Challenge: Learning the Potential of Morse Pairs from LMBTR Bulk Data\n",
        "\n",
        "Lets re-take the data from the MD we considered in the fist section. Instead of lookint to global descriptors, we are now loofing into local ones for 1000 diferent atoms, each one from one diferent frame of our MD. Additionaly, the atomic contribution for the total potential energy is also done.\n",
        "\n",
        "We will use this data to train a potential energy model using a ANN, similar to the one in the regression notebook. We will test this in some artificialy generated data for a dimer of copper atoms, for wich we know the ground trouth of the interaction energy (the Morse Potential). Try to play a little bit with the NN structure and training. The initial implementation we are giving here is ok, but can be inpruved. How can it be done?"
      ],
      "metadata": {
        "id": "EBfH7VyC7eLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.load('/content/Notebooks/Oleron Solidification School 2024/data/MD_melting/lmbtr_Cuatoms_i_i.npy')\n",
        "y = np.load('/content/Notebooks/Oleron Solidification School 2024/data/MD_melting/pe_Cuatoms_i_i.npy')\n",
        "\n",
        "#X = np.load('/content/lmbtr_Cuatoms_dimer.npy')\n",
        "#y = np.load('/content/pe_Cuatoms_dimer.npy')\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "RNG_SEED = 171\n",
        "np.random.seed(RNG_SEED)\n",
        "N_sample = len(X)\n",
        "N_train = int(0.70 * N_sample)\n",
        "N_test = N_sample\n",
        "\n",
        "ind_train = np.random.choice(range(N_sample), size = N_train, replace = False) #create the training set\n",
        "ind_test = np.array(list(i for i in range(N_sample) if i not in ind_train)) #create the testset with remaining elements not in ind list\n",
        "\n",
        "x_data = X\n",
        "y_data = y\n",
        "\n",
        "#pick up points in the indices' array\n",
        "x_train = x_data[np.array(ind_train)]\n",
        "y_train = y_data[np.array(ind_train)]\n",
        "print('train set size: ', y_train.shape)\n",
        "\n",
        "#pick up points in the indices' array not in the train (very important to avoid bias in the testing\n",
        "x_test = x_data[np.array(ind_test)]\n",
        "y_test = y_data[np.array(ind_test)]\n",
        "print('test set size: ', y_test.shape)"
      ],
      "metadata": {
        "id": "w7Ay6ulnFLZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the ANN model/archtecture\n",
        "# Question: How many parameters this function has? We have 1000 data points...\n",
        "NN=10\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, NN)\n",
        "        self.fc2 = nn.Linear(NN, NN)\n",
        "        self.fc3 = nn.Linear(NN, NN)\n",
        "        self.fc4 = nn.Linear(NN, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Create the ANN model\n",
        "ann = ANN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(ann.parameters(), lr=0.001)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "x_data = torch.tensor(x_data, dtype=torch.float32)\n",
        "y_data = torch.tensor(y_data, dtype=torch.float32)\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "\n",
        "val_plot=[]\n",
        "loss_plot=[]\n",
        "# Train the ANN model\n",
        "for epoch in range(5000):\n",
        "    # Forward pass\n",
        "    y_pred = ann(x_train)\n",
        "    y_val = ann(x_test)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    val = loss_fn(y_val, y_test)\n",
        "    val_plot.append(val.item())\n",
        "    loss_plot.append(loss.item())\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch+1) % 100 == 0:\n",
        "      print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Val: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "################################################\n",
        "#\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.set_title(\"Multi-layer Perceptron\")\n",
        "ax.set_xlabel(\"Iterations\")\n",
        "ax.set_ylabel(\"Loss\")\n",
        "ax.set_xscale('log')\n",
        "ax.set_yscale('log')\n",
        "\n",
        "ax.plot(loss_plot,label=\"Train set\")\n",
        "ax.plot(val_plot,label=\"Validation set\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uREOiQC5IJmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ase import Atoms\n",
        "from dscribe.descriptors import LMBTR\n",
        "\n",
        "# Create a series of ase atoms objects containing two Cu atoms\n",
        "# Question: Is this data in the same class of the training? We expect our model to perform good here?\n",
        "atoms = []\n",
        "N=100\n",
        "dist=[1.5+i*(5.0-1.5)/float(N) for i in range(N)]\n",
        "for d in dist:\n",
        "    dimmer = Atoms('Cu2', positions=[(0, 0, 0), (0, 0, d)])\n",
        "    atoms.append(dimmer)\n",
        "\n",
        "# Use dscribe to compute the LMBTR for the first atom in each object\n",
        "lmbtr = LMBTR(\n",
        "    species=[\"Cu\"],\n",
        "    geometry={\"function\": \"distance\"},\n",
        "    grid={\"min\": 1.5, \"max\": 5, \"n\": 100, \"sigma\": 0.05},\n",
        "    weighting={\"function\": \"exp\", \"scale\": 0.5, \"threshold\": 1e-3},\n",
        "    periodic=True,\n",
        "    normalization=\"l2\")\n",
        "\n",
        "discripts=[]\n",
        "for dimmer in atoms:\n",
        "    disc=lmbtr.create(dimmer, centers=[0])\n",
        "    discripts.append(disc[0][100:200])\n",
        "\n",
        "# Convert the sample data to a PyTorch tensor\n",
        "x_sample = torch.tensor(discripts, dtype=torch.float32)\n",
        "\n",
        "# Predict the output for the sample data\n",
        "y_pred_samples = ann(x_sample).detach().numpy()\n",
        "y_pred_samples=[y_pred_samples[i]-y_pred_samples[86] for i in range(len(y_pred_samples))] #apply rcut in 4.5 (dist[86])\n",
        "\n",
        "\n",
        "# Computing real value:\n",
        "D = 0.3429/23.0;r = 2.866; a = 1.3588; rcut=4.5\n",
        "delta=2.0*D*(np.exp(-2*a*(rcut-r))-2*np.exp(-a*(rcut-r)))\n",
        "V_real = [2.0*D*(np.exp(-2*a*(d-r))-2*np.exp(-a*(d-r)))-delta  for d in dist]\n",
        "\n",
        "plt.plot(dist,y_pred_samples,'o',label=\"Predicted\")\n",
        "plt.plot(dist,V_real,'o',label=\"Real\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QHRLn1JsI_od"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}