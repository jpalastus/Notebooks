{"cells":[{"cell_type":"markdown","metadata":{"id":"_wN4OpTNiT7I"},"source":["# Hands-on 3: Regression\n"]},{"cell_type":"markdown","metadata":{"id":"WuPs0ogyiT7L"},"source":["## Fitting\n","\n","As we approach the problem of regression in tha machine learning perspective, lets firsttry to gain intuition about how fitting curves to data work.\n","\n","Lets take, to begin, the simple task of fitting data with polynomials of different order. Formally, this goes under the name of polynomial regression.\n"]},{"cell_type":"markdown","source":["\n","### Some theory on the prediction problem\n","\n","Lets consider a series of mesurements that gives rise to data $(x,y)$. In the machine learning language, we say that $x$ are the *data points* and $y$ are the *labels*, so we call $(x,y)$ as *labeled data*. One can model the noise in those mesurements using\n","$$\n","    y_i= f(x_i) + \\eta_i,\n","$$\n","where $f(x_i)$ is some fixed, but (possibly unknown) function, and $\\eta_i$ is a Gaussian, uncorrelate noise variable with average zero.\n","\n","In a more physics-friendly language, this is equivalent to say that $x$ are the controll quantities (like setup pressure and temperature), while $y$ is the mesurable quantity (like the viscosity mesured in a fluid under that presure and temperature). $\\eta_i$ is in nthis analogy the aleatory error associated with the mesurement $i$ (systemic error is inexistent or included as part o $f(x_i)$ when we define $<\\eta_i>=0$). $f(x_i)$ is then the ground trouth behind the mesurements.\n","\n","To build a interpolation, we consider a family of functions, lets say $g_\\alpha(x,\\theta_\\alpha)$, that depend on some parameters $\\theta_\\alpha$. These functions respresent the class of models we are using to try to depict the $y_i$ and make predictions.\n","\n","To learn the parameters $\\boldsymbol{\\theta}$, we will train our models on a subset of $(x,y)$ we call *training set* and then test the effectiveness of the model on a different subset, the *test set*. The reason we must divide our data into a training and test dataset is that the point of machine learning is to make accurate predictions about new data we have not seen.\n","\n","Our task is to model the data with polynomials and make predictions about the new data that we have not seen. We will consider two qualitatively distinct situations:\n","\n","- In the first case, the process that generates the underlying data is in the model class we are using to make predictions. For polynomial regression, this means that the functions $f(x_i)$ are themselves polynomials.\n","- In the second case, our data lies outside our model class. In the case of polynomial regression, this could correspond to the case where the $f(x_i)$ is a 10-th order polynomial or an exponential, but $g_\\alpha(x,\\theta_\\alpha)$ are polynomials of order 1 or 3.\n","\n","In the exercises and discussion we consider 3 model classes:\n","- The case where the $g_\\alpha(x;\\theta_\\alpha)$ are all polynomials up to order 1 (linear models),\n","- The case where the $g_\\alpha(x;\\theta_\\alpha)$ are all polynomials up to order 3,\n","- The case where the $g_\\alpha(x;\\theta_\\alpha)$ are all polynomials up to order 10.\n","\n","One common measure of predictive  performance of our algorithm is to compare the predictions,$y_j^\\mathrm{pred}$, to the true values $y_j$. A commonly employed measure for this is the sum of the mean square-error (MSE) on the test set:\n","\n","$$\n","MSE= \\frac{1}{N_\\mathrm{test}}\\sum_{j=1}^{N_\\mathrm{test}} (y_j^\\mathrm{pred}-y_j)^2\n","$$\n"],"metadata":{"id":"6agcaURjkjyf"}},{"cell_type":"markdown","metadata":{"id":"TC-VDhmDiT7N"},"source":["## Creating a dataset and fitting the data with a model using a simple regression\n","\n","Python modules:\n","\n","https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n","\n","https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n","\n","We start for the true features by considering the case:\n","\n","$$\n","f(x)=2x.\n","$$\n","\n","Then the data is clearly generated by a model that is contained within all three model classes we are using to make predictions (linear models, third order polynomials, and tenth order polynomials).\n","\n","The first exercise\n","<ul>\n","<li> Generate two datasets (size $N=10$ and $N=100$) with the random process described above using $\\sigma=0.5$  \n","<li> in each case plot the dataset as scattered points as well as the reference function (true features)\n","<li> Perform a linear regression using linregress() of scipy.stats module. for the two datasets and discuss your results\n","<li> Perform a non-linear fitting with polynomials of order 3 and 10, making use of \"curve_fit\" module of oscipy.optimize: for the two datasets and discuss your results\n","<li> What if one applies the models outside the fitting range ? make some tests ?    \n","</ul>"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"aOdozo77iT7P"},"source":["### Plot the model and its noisy representation as a training data"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"N3oWH2OciT7O"},"outputs":[],"source":["#\n","# Import necessary modules\n","#\n","import numpy as np\n","import random\n","%matplotlib inline\n","\n","from scipy import stats\n","from scipy.optimize import curve_fit\n","\n","from sklearn import datasets, linear_model\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"KPtlKo0miT7P"},"outputs":[],"source":["##########################\n","# functions\n","##########################\n","\n","# define the noise\n","#\n","def gnoise(sigma, N):\n","    return sigma*np.random.randn(N)\n","\n","#Function that gives pi value? Here just for fun ^^\n","def pi():\n","    return 3.141592653589793\n","\n","#Define the fitting funtion\n","#\n","def fref(x):\n","    return 2*x\n","\n","def func3(x, a, b, c, d):\n","    return a*x**3 + b*x**2 + c*x + d\n","\n","def func10(x, a, b, c):\n","    return a*x**10 + b*x**5 + c*x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"rHCrTaRKiT7P"},"outputs":[],"source":["# Define The sample size and amplitude of the noise\n","N_sample=10\n","sigma_sample=0.5\n","x_min = 0\n","x_max = 1\n","\n","# Set a random seed to ensure reproducibility during the test of the code\n","RNG_SEED = 171\n","np.random.seed(seed=RNG_SEED)\n","\n","# discretize the x-axis in N_train points in a given interval\n","x_in_s10=np.linspace(x_min+0.01, x_max-0.01, N_sample)\n","\n","# create the reference function\n","y_ref_s10 = fref(x_in_s10)\n","y_in_s10 = y_ref_s10 + gnoise(sigma_sample, N_sample)\n","\n","# larger sample N =100\n","N_sample=100\n","\n","# discretize the x-axis in N_train points in a given interval\n","x_in_s100=np.linspace(x_min+0.01, x_max-0.01, N_sample)\n","\n","# create the reference function\n","y_ref_s100 = fref(x_in_s100)\n","y_in_s100 = y_ref_s100 + gnoise(sigma_sample, N_sample)\n","\n","# Draw the reference function and with Gaussian random noise\n","figure, (ax1,ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (14,5))\n","\n","ax1.plot(x_in_s10, y_ref_s10, color = 'red', label='Reference')\n","ax1.plot(x_in_s10, y_in_s10, \"o\", ms=6, alpha=0.5, label='Sample')\n","ax1.set_xlabel('x',fontsize = 16)\n","ax1.set_ylabel('y',fontsize = 16)\n","ax1.legend(fontsize=12)\n","\n","ax2.plot(x_in_s100, y_ref_s100, color = 'red', label='Reference')\n","ax2.plot(x_in_s100, y_in_s100, \"o\", ms=6, alpha=0.5, label='Sample')\n","ax2.set_xlabel('x',fontsize = 16)\n","ax2.set_ylabel('y',fontsize = 16)\n","ax2.legend(fontsize=12)\n","\n","plt.show()\n"]},{"cell_type":"markdown","source":["## Linear Regression"],"metadata":{"id":"isoqNFW01xs9"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"eqthvYydiT7Q"},"outputs":[],"source":["print('N= 10 sample\\n')\n","result = stats.linregress(x_in_s10,y_in_s10)\n","a10, b10, r_value10, p_value10, std_err10 = result.slope,result.intercept,result.rvalue,result.pvalue,result.stderr\n","print('Linear regression slope =', a10)\n","print('Linear regression intercept =', b10)\n","print('Linear regression r-value =', r_value10)\n","print('Linear regression p-value =', p_value10)\n","print('Linear regression standard error in the gradient =', std_err10)\n","\n","print('N= 100 sample\\n')\n","result = stats.linregress(x_in_s100,y_in_s100)\n","a100, b100, r_value100, p_value100, std_err100 = result.slope,result.intercept,result.rvalue,result.pvalue,result.stderr\n","print('Linear regression slope =', a100)\n","print('Linear regression intercept =', b100)\n","print('Linear regression r-value =', r_value100)\n","print('Linear regression p-value =', p_value100)\n","print('Linear regression standard error in the gradient =', std_err100)\n","\n","# Draw the reference function and with Gaussian random noise\n","figure, (ax1,ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (14,5))\n","\n","ax1.plot(x_in_s10, y_ref_s10, color = 'red', label='Reference')\n","ax1.plot(x_in_s10, a10*x_in_s10+b10, color = 'green', label='Regression')\n","ax1.plot(x_in_s10, y_in_s10, \"o\", ms=6, alpha=0.5, label='Sample')\n","ax1.set_xlabel('x',fontsize = 16)\n","ax1.set_ylabel('y',fontsize = 16)\n","ax1.legend(fontsize=12)\n","\n","ax2.plot(x_in_s100, y_ref_s100, color = 'red', label='Reference')\n","ax2.plot(x_in_s100, a100*x_in_s100+b100, color = 'green', label='Rregression')\n","ax2.plot(x_in_s100, y_in_s100, \"o\", ms=6, alpha=0.5, label='Sample')\n","ax2.set_xlabel('x',fontsize = 16)\n","ax2.set_ylabel('y',fontsize = 16)\n","ax2.legend(fontsize=12)\n","\n","plt.show()"]},{"cell_type":"markdown","source":["## Polinomial Regression\n"],"metadata":{"id":"vAxRjpRW166i"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"LQ4DGDU-iT7S"},"outputs":[],"source":["\n","# Order 3\n","popt3, pcov3 = curve_fit(func3, x_in_s10, y_in_s10)\n","print('Fitting coefficients: ',popt3)\n","print('Covariance matrix: \\n',pcov3)\n","# Order 10\n","popt10, pcov10 = curve_fit(func10, x_in_s10, y_in_s10)\n","print('Fitting coefficients: ',popt10)\n","print('Covariance matrix: \\n',pcov10)\n","\n","\n","Figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","ax1.plot(x_in_s10,y_in_s10, \"o\",ms=6, alpha = 0.5, label='Dataset')\n","ax1.plot(x_in_s10, y_ref_s10, color = 'red', label='Reference')\n","ax1.plot(x_in_s10, a10*x_in_s10+b10, color = 'blue', label='fit')\n","ax1.plot(x_in_s10, func3(x_in_s10, *popt3), color = 'green', label='fit: a=%5.3f, b=%5.3f, c=%5.3f, d=%5.3f' % tuple(popt3))\n","ax1.set_xlabel(r'$x$', fontsize = 16)\n","ax1.set_ylabel(r'$y$', fontsize = 16)\n","ax1.legend(fontsize = 10)\n","\n","ax2.plot(x_in_s10,y_in_s10, \"o\",ms=6, alpha = 0.5, label='Dataset')\n","ax2.plot(x_in_s10, y_ref_s10, color = 'red', label='Reference')\n","ax2.plot(x_in_s10, a10*x_in_s10+b10, color = 'blue', label='fit')\n","ax2.plot(x_in_s10, func10(x_in_s10, *popt10), color = 'green', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt10))\n","ax2.set_xlabel(r'$x$', fontsize = 16)\n","ax2.set_ylabel(r'$y$', fontsize = 16)\n","ax2.legend(fontsize = 10)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_EYbOwiiT7S"},"outputs":[],"source":["# Redo this for a sample size of 100 points\n","# Define The Training Data size and amplitude of the noise\n","N_sample_100=100\n","sigma_sample=0.5\n","x_min = 0\n","x_max = 1\n","\n","# Set a random seed to ensure reproducibility during the test of the code\n","RNG_SEED = 42\n","np.random.seed(seed=RNG_SEED)\n","\n","# discretize the x-axis in N_train points in a given interval\n","\n","x_in_s100=np.linspace(x_min+0.01, x_max-0.01, N_sample_100)\n","\n","# create the reference function\n","y_ref_s100 = fref(x_in_s100)\n","y_in_s100 = y_ref_s100 + gnoise(sigma_sample, N_sample_100)\n","\n","\n","# Draw the reference function and with Gaussian random noise\n","\n","plt.plot(x_in_s100, y_ref_s100, color = 'red', label='Reference')\n","plt.plot(x_in_s100, y_in_s100, \"o\", ms=6, alpha=0.5, label='Dataset')\n","plt.xlabel('x',fontsize = 16)\n","plt.ylabel('y',fontsize = 16)\n","plt.legend(fontsize=12)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"K-92lllJiT7T"},"outputs":[],"source":["result = stats.linregress(x_in_s100,y_in_s100)\n","a, b, r_value, p_value, std_err = result.slope,result.intercept,result.rvalue,result.pvalue,result.stderr\n","print('Linear regression slope =', a)\n","print('Linear regression intercept =', b)\n","print('Linear regression r-value =', r_value)\n","print('Linear regression p-value =', p_value)\n","print('Linear regression standard error in the gradient =', std_err)\n","\n","# Plot the result\n","xmin,xmax,ymin,ymax = 0.,10.0,0.,4.\n","plt.plot(x_in_s100,y_in_s100, \"o\",ms=6, alpha = 0.5, label='Dataset')\n","plt.plot(x_in_s100, y_ref_s100, color = 'red', label='Reference')\n","plt.plot(x_in_s100, a*x_in_s100+b, color = 'blue', label='fit')\n","plt.xlabel(r'$x$', fontsize = 16)\n","plt.ylabel(r'$y$', fontsize = 16)\n","plt.legend(fontsize=12)\n","plt.show()\n","\n","# non linear Regression\n","\n","# Order 3\n","popt3, pcov3 = curve_fit(func3, x_in_s100, y_in_s100)\n","print('Fitting coefficients: ',popt3)\n","print('Covariance matrix: \\n',pcov3)\n","# Order 10\n","popt10, pcov10 = curve_fit(func10, x_in_s100, y_in_s100)\n","print('Fitting coefficients: ',popt10)\n","print('Covariance matrix: \\n',pcov10)\n","\n","\n","Figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","ax1.plot(x_in_s100,y_in_s100, \"o\",ms=6, alpha = 0.5, label='Dataset')\n","ax1.plot(x_in_s100, y_ref_s100, color = 'red', label='Reference')\n","ax1.plot(x_in_s100, a100*x_in_s100+b100, color = 'blue', label='fit linear')\n","ax1.plot(x_in_s100, func3(x_in_s100, *popt3), color = 'green', label='fit: a=%5.3f, b=%5.3f, c=%5.3f, d=%5.3f' % tuple(popt3))\n","ax1.set_xlabel(r'$x$', fontsize = 16)\n","ax1.set_ylabel(r'$y$', fontsize = 16)\n","ax1.legend(fontsize = 10)\n","\n","ax2.plot(x_in_s100,y_in_s100, \"o\",ms=6, alpha = 0.5, label='Dataset')\n","ax2.plot(x_in_s100, y_ref_s100, color = 'red', label='Reference')\n","ax2.plot(x_in_s100, a100*x_in_s100+b100, color = 'blue', label='fit linear')\n","ax2.plot(x_in_s100, func10(x_in_s100, *popt10), color = 'green', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt10))\n","ax2.set_xlabel(r'$x$', fontsize = 16)\n","ax2.set_ylabel(r'$y$', fontsize = 16)\n","ax2.legend(fontsize = 10)\n","plt.show()"]},{"cell_type":"markdown","source":["## Extrapolation of the model"],"metadata":{"id":"J5PCc9YD2TRH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zk61f38xiT7T"},"outputs":[],"source":["# extrapolation :\n","\n","x_extra = np.linspace(x_min-0.5, x_max+0.5, N_sample)\n","\n","Figure, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(7,5))\n","ax1.plot(x_in_s100,y_in_s100, \"o\",ms=6, alpha = 0.5, label='Dataset')\n","ax1.plot(x_extra, 2*x_extra, color = 'red', label='Reference')\n","ax1.plot(x_extra, a*x_extra+b, color = 'blue', label='fit linear')\n","ax1.plot(x_extra, func3(x_extra, *popt3), color = 'green', label='fit: a=%5.3f, b=%5.3f, c=%5.3f, d=%5.3f' % tuple(popt3))\n","ax1.plot(x_extra, func10(x_extra, *popt10), color = 'orange', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt10))\n","\n","ax1.set_ylim(ymin = -3)\n","ax1.set_xlabel(r'$x$', fontsize = 16)\n","ax1.set_ylabel(r'$y$', fontsize = 16)\n","ax1.legend(fontsize = 10)\n"]},{"cell_type":"markdown","metadata":{"id":"_9wRyef1iT7U"},"source":["## Fitting vs. predicting when the data is not in the model class\n","\n"]},{"cell_type":"markdown","source":["Python module:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n","\n","Thus far, we have considered the case where the data is generated using a model contained in the model class. Now consider  $f(x)=2x-10x^5+15x^{10}$. *Notice that the for linear and third-order polynomial the true model $f(x)$ is not contained in model class $g_\\alpha(x)$* .\n","Generally, the polynomial regression models of degree $n$ can be written as  \n","\n","$$\n","y_i =  \\beta_0+\\beta_1x_i+\\beta_2x^2_i+\\cdots+\\beta_nx^n_i+\\varepsilon_i, \\textrm{for } i = 1,\\cdots, m,\n","$$\n","\n","where $\\varepsilon_i$ is the résidual error.  Interestingly, it is seen that these models are linear from the point of view of estimation, since the regression function is linear in terms of the unknown parameters $\\beta_0$+$\\beta_1$, $\\cdots$ +$\\beta_n$ as they can be expressed in the matrix form for $m<n$ points $\\{ x_i,y_i\\}$\n","\n","$$\n","\\begin{bmatrix}y_1\\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}=\\begin{bmatrix} 1 & x_1 & x^2_1 & \\cdots & x^n_1 \\\\ 1 & x_2 & x^2_2 & \\cdots & x^n_2 \\\\ \\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\ 1 & x_m & x^2_m & \\cdots & x^n_m   \\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_m\\end{bmatrix}+\\begin{bmatrix}\\varepsilon_1\\\\ \\varepsilon_2\\\\ \\vdots \\\\ \\varepsilon_m\\end{bmatrix}\n","$$\n","\n","This is exactly what the module 'PolynomialFeatures' does *i.e.* tranforming a set of input points $x_i$ for a regresion of degree $n$, therefore creating $m$ feature vectors $\\mathbf{X_i}$ of dimention $n+1$ in a matrix form (see the documentation).\n","\n","<ul>\n","<li> Repeat the exercises above fitting and predicting for $f(x)=2x-10x^5+15x^{10}$ for $N_{\\rm{train}}=10,100$ and $\\sigma=0,3$. Record your observations.\n","<li> Do better fits lead to better predictions?\n","<li> What is the relationship between the true model for generating the data and the model class that has the most predictive power? How is this related to the model complexity? How does this depend on the number of data points $N_{\\mathrm{train}}$ and $\\sigma$?\n","<li> Summarize what you think you learned about the relationship of knowing the true model class and predictive power."],"metadata":{"id":"WTA05_hF21Mv"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"kMaPD1GUiT7Y"},"outputs":[],"source":["# create a sample with the new model.\n","RNG_SEED = 42\n","np.random.seed(seed=RNG_SEED)\n","\n","N_sample = 300\n","sigma_sample = 0.3\n","x_min = 0\n","x_max = 1\n","\n","x_sample=np.linspace(x_min-0.1, x_max+0.1, N_sample) # regular discretisation of the x values for the sample\n","y_ref_sample = func10(x_sample,5,10,2)\n","y_sample = y_ref_sample + gnoise(sigma_sample, N_sample)\n","\n","# Create a dataset for train set and test set out of the sample with 200 elements\n","# so that we suppose that 100 point are unknown\n","N_data =  200\n","\n","RNG_SEED = 142\n","np.random.seed(RNG_SEED)\n","ind = np.random.choice(range(50,250),size = N_data, replace = False)\n","\n","x_data = x_sample[np.array(ind)]\n","y_data = y_sample[np.array(ind)]\n","print('Dataset size: ', len(y_data))\n","\n","# Define the training set by picking up random N_train values in the (x_sample, y_sample) and the test set\n","\n","N_train = 10\n","\n","RNG_SEED = 242\n","random.seed(RNG_SEED)\n","\n","ind_train = np.random.choice(range(200), size = N_train, replace = False) # Here the x domain is reduced for the train so that\n","                                              # so that we can observe extrapolation ability of the models\n","ind_test = np.array(list(i for i in range(200) if i not in ind_train)) #create the textset with remaining elements not in ind list\n","\n","\n","#pick up points in the indices' array\n","x_train = x_data[np.array(ind_train)]\n","y_train = y_data[np.array(ind_train)]\n","print('train set size: ', len(y_train))\n","\n","#pick up points in the indices' array not in the train (very important to avoid bias in the testing\n","x_test = x_data[np.array(ind_test)]\n","y_test = y_data[np.array(ind_test)]\n","print('test set size: ', len(y_test))\n","\n","\n","# ---------------------------------------------\n","#create the polynomial model of degree 3\n","# ---------------------------------------------\n","\n","poly3 = PolynomialFeatures(degree=3)\n","\n","# Construct polynomial features\n","X3 = poly3.fit_transform(x_train[:,np.newaxis])\n","\n","# Perform the training\n","clf3 = linear_model.LinearRegression()\n","trained_modelP3 = clf3.fit(X3,y_train)\n","\n","#Predict on the whole sample\n","Xs=poly3.fit_transform(x_sample[:,np.newaxis])\n","y3_predict = trained_modelP3.predict(Xs)\n","\n","print(\"__________________________\")\n","print(\"polynomial model degree 3\")\n","print(\"__________________________\")\n","print('coefficients:', trained_modelP3.coef_)\n","print('train set size: {}'.format(len(y_train)))\n","print('test set size: {}'.format(len(y_test)))\n","\n","Xt=poly3.fit_transform(x_test[:,np.newaxis])\n","y3_predict_test = trained_modelP3.predict(Xt)\n","\n","# The mean squared error\n","print('Mean squared error E_out: %.8f'\n","      % mean_squared_error(y_test, y3_predict_test))\n","# The coefficient of determination: 1 is perfect prediction\n","print('Coefficient of determination r2_out: %.8f'\n","      % r2_score(y_test, y3_predict_test))\n","\n","# Fifth order polynomial in case  you want to try it out\n","#poly5 = PolynomialFeatures(degree=5)\n","#X = poly5.fit_transform(x[:,np.newaxis])\n","#clf5 = linear_model.LinearRegression()\n","#clf5.fit(X,y)\n","\n","#Xplot=poly5.fit_transform(xplot[:,np.newaxis])\n","#plt.plot(xplot, clf5.predict(Xplot), 'r--',linewidth=1)\n","\n","\n","# ---------------------------------------------\n","#create the polynomial model of degree 10\n","# ---------------------------------------------\n","\n","poly10 = PolynomialFeatures(degree=10)\n","\n","# Construct polynomial features\n","X10 = poly10.fit_transform(x_train[:,np.newaxis])\n","\n","# Perform the training\n","clf10 = linear_model.LinearRegression()\n","trained_modelP10 = clf10.fit(X10,y_train)\n","\n","#Predict on the whole sample\n","Xs=poly10.fit_transform(x_sample[:,np.newaxis])\n","y10_predict = clf10.predict(Xs)\n","\n","print(\"__________________________\")\n","print(\"polynomial model degree 10\")\n","print(\"__________________________\")\n","print('coefficients:', trained_modelP10.coef_)\n","print('train set size: {}'.format(len(y_train)))\n","print('test set size: {}'.format(len(y_test)))\n","\n","Xt=poly10.fit_transform(x_test[:,np.newaxis])\n","y10_predict_test = trained_modelP10.predict(Xt)\n","\n","# The mean squared error\n","print('Mean squared error E_out: %.8f'\n","      % mean_squared_error(y_test, y10_predict_test))\n","# The coefficient of determination: 1 is perfect prediction\n","print('Coefficient of determination r2_out: %.8f'\n","      % r2_score(y_test, y10_predict_test))\n","\n","\n","figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","ax1.plot(y_test, y_test, color = 'red', label='Test set')\n","ax1.plot(y_test, y3_predict_test, \"o\", ms=6, color = 'blue', alpha = 0.5, label='Predicted by the model')\n","\n","ax1.set_title('Polynomial model degree 3, N_train = 100', fontsize = 14)\n","ax1.set_xlabel('y (test)',fontsize = 16)\n","ax1.set_ylabel('y (predict)',fontsize = 16)\n","ax1.legend(fontsize=12)\n","\n","ax2.plot(y_test, y_test, color = 'red', label='Test set')\n","ax2.plot(y_test, y10_predict_test, \"o\", ms=6, color = 'blue', alpha = 0.5, label='Predicted by the model')\n","\n","ax2.set_title('Polynomial model degree 10, N_train = 100', fontsize = 14)\n","ax2.set_xlabel('y (test)',fontsize = 16)\n","ax2.set_ylabel('y (predict)',fontsize = 16)\n","ax2.legend(fontsize=12)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nONowMSciT7Y"},"outputs":[],"source":["figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","\n","ax1.plot(x_sample, y_sample, 'o', ms=6, color = 'blue', alpha = 0.1, label='Sample')\n","ax1.plot(x_data, y_data, 'o', ms=4, color = 'yellow', alpha = 0.3, label='Dataset')\n","ax1.plot(x_train, y_train, 'x', ms=8, color = 'red', alpha = 1, label='Train set')\n","ax1.plot(x_sample, y3_predict, color = 'green', label='prediction of Poly 3')\n","ax1.plot(x_sample, y10_predict, color = 'orange', label='prediction of Poly 10')\n","\n","ax1.set_xlabel('x',fontsize = 16)\n","ax1.set_ylabel('y',fontsize = 16)\n","ax1.set_ylim(ymin=-7,ymax=33)\n","ax1.legend(fontsize=10)\n","\n","ax2.plot(x_train, y_train, 'x', ms=8, color = 'red', alpha = 1, label='Train set')\n","ax2.plot(x_sample, y3_predict, color = 'green', label='prediction of Poly 3')\n","ax2.plot(x_sample, y10_predict, color = 'orange', label='prediction of Poly 10')\n","\n","ax2.set_xlabel('x',fontsize = 16)\n","ax2.set_ylabel('y',fontsize = 16)\n","ax2.set_ylim(ymin=-5,ymax=15)\n","ax2.legend(fontsize=10)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qROQvYoiT7Y"},"outputs":[],"source":["# Polynomial Regression larger sample\n","\n","print('Dataset size: ', len(y_data))\n","\n","# Define the training set by picking up random N_train values in the (x_sample, y_sample) and the test set\n","\n","N_train = 100\n","\n","RNG_SEED = 242\n","random.seed(RNG_SEED)\n","\n","ind_train = np.random.choice(range(200), size = N_train, replace = False) # Here the x domain is reduced for the train so that\n","                                              # so that we can observe extrapolation ability of the models\n","ind_test = np.array(list(i for i in range(200) if i not in ind_train)) #create the textset with remaining elements not in ind list\n","\n","\n","#pick up points in the indices' array\n","x_train = x_data[np.array(ind_train)]\n","y_train = y_data[np.array(ind_train)]\n","print('train set size: ', len(y_train))\n","\n","#pick up points in the indices' array not in the train (very important to avoid bias in the testing\n","x_test = x_data[np.array(ind_test)]\n","y_test = y_data[np.array(ind_test)]\n","print('test set size: ', len(y_test))\n","\n","\n","# ---------------------------------------------\n","#create the polynomial model of degree 3\n","# ---------------------------------------------\n","\n","poly3 = PolynomialFeatures(degree=3)\n","\n","# Construct polynomial features\n","X3 = poly3.fit_transform(x_train[:,np.newaxis])\n","\n","# Perform the training\n","clf3 = linear_model.LinearRegression()\n","trained_modelP3 = clf3.fit(X3,y_train)\n","\n","#Predict on the whole sample\n","Xs=poly3.fit_transform(x_sample[:,np.newaxis])\n","y3_predict = trained_modelP3.predict(Xs)\n","\n","print(\"__________________________\")\n","print(\"polynomial model degree 3\")\n","print(\"__________________________\")\n","print('coefficients:', trained_modelP3.coef_)\n","print('train set size: {}'.format(len(y_train)))\n","print('test set size: {}'.format(len(y_test)))\n","\n","Xt=poly3.fit_transform(x_test[:,np.newaxis])\n","y3_predict_test = trained_modelP3.predict(Xt)\n","\n","# The mean squared error\n","print('Mean squared error E_out: %.8f'\n","      % mean_squared_error(y_test, y3_predict_test))\n","# The coefficient of determination: 1 is perfect prediction\n","print('Coefficient of determination r2_out: %.8f'\n","      % r2_score(y_test, y3_predict_test))\n","\n","# Fifth order polynomial in case  you want to try it out\n","#poly5 = PolynomialFeatures(degree=5)\n","#X = poly5.fit_transform(x[:,np.newaxis])\n","#clf5 = linear_model.LinearRegression()\n","#clf5.fit(X,y)\n","\n","#Xplot=poly5.fit_transform(xplot[:,np.newaxis])\n","#plt.plot(xplot, clf5.predict(Xplot), 'r--',linewidth=1)\n","\n","\n","# ---------------------------------------------\n","#create the polynomial model of degree 10\n","# ---------------------------------------------\n","\n","poly10 = PolynomialFeatures(degree=10)\n","\n","# Construct polynomial features\n","X10 = poly10.fit_transform(x_train[:,np.newaxis])\n","\n","# Perform the training\n","clf10 = linear_model.LinearRegression()\n","trained_modelP10 = clf10.fit(X10,y_train)\n","\n","#Predict on the whole sample\n","Xs=poly10.fit_transform(x_sample[:,np.newaxis])\n","y10_predict = clf10.predict(Xs)\n","\n","print(\"__________________________\")\n","print(\"polynomial model degree 10\")\n","print(\"__________________________\")\n","print('coefficients:', trained_modelP10.coef_)\n","print('train set size: {}'.format(len(y_train)))\n","print('test set size: {}'.format(len(y_test)))\n","\n","Xt=poly10.fit_transform(x_test[:,np.newaxis])\n","y10_predict_test = trained_modelP10.predict(Xt)\n","\n","# The mean squared error\n","print('Mean squared error E_out: %.8f'\n","      % mean_squared_error(y_test, y10_predict_test))\n","# The coefficient of determination: 1 is perfect prediction\n","print('Coefficient of determination r2_out: %.8f'\n","      % r2_score(y_test, y10_predict_test))\n","\n","\n","figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","ax1.plot(y_test, y_test, color = 'red', label='Test set')\n","ax1.plot(y_test, y3_predict_test, \"o\", ms=6, color = 'blue', alpha = 0.5, label='Predicted by the model')\n","\n","ax1.set_title('Polynomial model degree 3, N_train = 100', fontsize = 14)\n","ax1.set_xlabel('y (test)',fontsize = 16)\n","ax1.set_ylabel('y (predict)',fontsize = 16)\n","ax1.legend(fontsize=12)\n","\n","ax2.plot(y_test, y_test, color = 'red', label='Test set')\n","ax2.plot(y_test, y10_predict_test, \"o\", ms=6, color = 'blue', alpha = 0.5, label='Predicted by the model')\n","\n","ax2.set_title('Polynomial model degree 10, N_train = 100', fontsize = 14)\n","ax2.set_xlabel('y (test)',fontsize = 16)\n","ax2.set_ylabel('y (predict)',fontsize = 16)\n","ax2.legend(fontsize=12)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guxmrohiiT7Z"},"outputs":[],"source":["figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","\n","ax1.plot(x_sample, y_sample, 'o', ms=6, color = 'blue', alpha = 0.1, label='Sample')\n","ax1.plot(x_data, y_data, 'o', ms=4, color = 'yellow', alpha = 0.3, label='Dataset')\n","ax1.plot(x_train, y_train, 'o', ms=3, color = 'red', alpha = 0.3, label='Train set')\n","ax1.plot(x_sample, y3_predict, color = 'green', label='prediction of Poly 3')\n","ax1.plot(x_sample, y10_predict, color = 'orange', label='prediction of Poly 10')\n","\n","ax1.set_xlabel('x',fontsize = 16)\n","ax1.set_ylabel('y',fontsize = 16)\n","ax1.set_ylim(ymin=-7,ymax=33)\n","ax1.legend(fontsize=10)\n","\n","ax2.plot(x_train, y_train, 'o', ms=6, color = 'red', alpha = 0.5, label='Train set')\n","ax2.plot(x_sample, y3_predict, color = 'green', label='prediction of Poly 3')\n","ax2.plot(x_sample, y10_predict, color = 'orange', label='prediction of Poly 10')\n","\n","ax2.set_xlabel('x',fontsize = 16)\n","ax2.set_ylabel('y',fontsize = 16)\n","ax2.set_ylim(ymin=-5,ymax=15)\n","ax2.legend(fontsize=10)\n","\n","plt.show()"]},{"cell_type":"markdown","source":["## Regression with an Artificial Neural Network\n","\n","Artificial Neural Networks (ANNs) are a very commun approach in modern machine learning. Inspired by the structure and function of the human brain, ANNs are designed to recognize patterns, make predictions, and generalize from data. To make here a first contact with the topic, we will explore how to fit an ANN to the data above using first the simples definition of Multilayer Perceptroin in scikit-learn."],"metadata":{"id":"76VIYELM15pR"}},{"cell_type":"code","source":["# Re-creating a sample with the 10th degree polinomial above.\n","RNG_SEED = 171\n","np.random.seed(seed=RNG_SEED)\n","\n","N_sample = 300\n","sigma_sample = 0.3\n","x_min = 0\n","x_max = 1\n","\n","x_sample=np.linspace(x_min-0.1, x_max+0.1, N_sample) # regular discretisation of the x values for the sample\n","y_ref_sample = func10(x_sample,5,10,2)\n","y_sample = y_ref_sample + gnoise(sigma_sample, N_sample)\n","\n","# Create a dataset for train set and test set out of the sample with 200 elements\n","# so that we suppose that 100 point are unknown\n","N_data =  200\n","\n","RNG_SEED = 42\n","np.random.seed(RNG_SEED)\n","ind = np.random.choice(range(50,250),size = N_data, replace = False)\n","\n","x_data = x_sample[np.array(ind)]\n","y_data = y_sample[np.array(ind)]\n","print('Dataset size: ', len(y_data))\n","\n","\n","\n"],"metadata":{"id":"LgEm1HD915GR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Making a ANN learn using the Multilayer Perseptron from sklearn\n","\n","\n","Multilayer perseptrons are fully conected ANNs that consist of interconnected layers of nodes, or \"neurons.\" These neurons are arranged in layers:\n","\n"," - Input Layer: The input layer receives the data directly. Each neuron in this layer corresponds to one feature of the data.\n"," - Hidden Layers: Between the input and output layers are one or more hidden layers, where the neurons process the input data using weighted connections.\n"," - Output Layer: The output layer produces the final prediction or classification.\n","\n","ANNs are useful in many contexts because they are considered universal approximators, meaning they can theoretically learn any function given sufficient data and computational resources.\n","\n"],"metadata":{"id":"qmmmb1W6AFaP"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.neural_network import MLPRegressor\n","\n","\n","\n","################################################\n","#\n","ann = MLPRegressor(hidden_layer_sizes=(32,32), activation='relu', solver='adam', learning_rate='adaptive', max_iter=1000, early_stopping=True, validation_fraction=0.2)\n","ann.fit(x_data.reshape(-1, 1), y_data)\n","evol_loss = ann.loss_curve_\n","evol_val_loss = ann.validation_scores_\n","ypred_data = ann.predict(x_data.reshape(-1, 1))\n","ypred_samples = ann.predict(x_sample.reshape(-1, 1))\n","\n","\n","\n","################################################\n","#\n","figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","ax1.plot(y_data, y_data, color = 'red', label='Test set')\n","ax1.plot(y_data, ypred_data, \"o\", ms=6, color = 'blue', alpha = 0.5, label='Predicted by the model')\n","\n","ax1.set_title('ANN, N_train = 160, N_val = 40', fontsize = 14)\n","ax1.set_xlabel('y (test)',fontsize = 16)\n","ax1.set_ylabel('y (predict)',fontsize = 16)\n","ax1.legend(fontsize=12)\n","\n","ax2.plot(x_sample, y_sample, 'o', ms=6, color = 'blue', alpha = 0.1, label='Sample')\n","ax2.plot(x_data, y_data, 'o', ms=4, color = 'yellow', alpha = 0.3, label='Dataset')\n","ax2.plot(x_sample, ypred_samples, color = 'orange', label='prediction of ANN')\n","\n","ax2.set_xlabel('x',fontsize = 16)\n","ax2.set_ylabel('y',fontsize = 16)\n","ax2.set_ylim(ymin=-7,ymax=33)\n","ax2.legend(fontsize=10)\n","\n","plt.show()\n","\n","################################################\n","#\n","fig, ax = plt.subplots(figsize=(12, 6))\n","ax.set_title(\"Multi-layer Perceptron\")\n","ax.set_xlabel(\"Iterations\")\n","ax.set_ylabel(\"Value\")\n","ax.plot(evol_loss,label=\"Train Set Loss\")\n","ax.plot(evol_val_loss,label=\"Test Set R$^2$\")\n","ax.legend()\n","plt.show()\n"],"metadata":{"id":"4dD71_Rh30ic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training an ANN with PyTorch\n","\n","In the previous part, we used scikit-learn to implement a simple MLP. While scikit-learn is an excellent tool for many machine learning tasks, it has some limitations when it comes to more advanced neural network architectures and customizations. To overcome those and gain more flexibility, we will use PyTorch, a powerful and popular deep learning framework. PyTorch provides dynamic computational graphs and extensive support for building and training complex neural networks, making it a preferred choice for many researchers and practitioners.\n","\n","We will train an ANN to perform a regression task on the function $sin⁡(x)$ over the interval $[0,2\\pi]$. After finishing, we recomend that you spend some time trying to play with this exemple, interpreting the loss/validation curve and check how it changes with the amount of data you give and the complexity of the ANN.\n","\n"],"metadata":{"id":"kiUcsNWIA-CJ"}},{"cell_type":"code","source":["# Re-creating a sample with the 10th degree polinomial above.\n","RNG_SEED = 171233\n","np.random.seed(seed=RNG_SEED)\n","\n","N_sample = 100\n","N_outside =10\n","N_train = 10\n","sigma_sample = 0.1\n","x_min = 0\n","x_max = 2.0*pi()\n","\n","x_sample=np.linspace(x_min-0.1, x_max+0.1, N_sample) # regular discretisation of the x values for the sample\n","y_ref_sample = np.sin(x_sample)\n","y_sample = y_ref_sample + gnoise(sigma_sample, N_sample)\n","\n","# Create a dataset for train set and test set out of the sample with 200 elements\n","# so that we suppose that 100 point are unknown\n","N_data =  N_sample-2*N_outside\n","\n","RNG_SEED = 42\n","np.random.seed(RNG_SEED)\n","ind = np.random.choice(range(N_outside,N_sample-N_outside),size = N_data, replace = False)\n","\n","x_data = x_sample[np.array(ind)]\n","y_data = y_sample[np.array(ind)]\n","print('Dataset size: ', len(y_data))\n","\n","RNG_SEED = 242\n","random.seed(RNG_SEED)\n","\n","ind_train = np.random.choice(range(N_data), size = N_train, replace = False) # Here the x domain is reduced for the train so that\n","                                              # so that we can observe extrapolation ability of the models\n","ind_test = np.array(list(i for i in range(N_data) if i not in ind_train)) #create the textset with remaining elements not in ind list\n","\n","\n","#pick up points in the indices' array\n","x_train = x_data[np.array(ind_train)]\n","y_train = y_data[np.array(ind_train)]\n","print('train set size: ', len(y_train))\n","\n","#pick up points in the indices' array not in the train (very important to avoid bias in the testing\n","x_test = x_data[np.array(ind_test)]\n","y_test = y_data[np.array(ind_test)]\n","print('test set size: ', len(y_test))\n","\n","###########################################"],"metadata":{"id":"fP0orG6GA9hS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define the ANN model/archtecture\n","NN=10\n","class ANN(nn.Module):\n","    def __init__(self):\n","        super(ANN, self).__init__()\n","        self.fc1 = nn.Linear(1, NN)\n","        self.fc2 = nn.Linear(NN, NN)\n","        self.fc3 = nn.Linear(NN, NN)\n","        self.fc4 = nn.Linear(NN, 1)\n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        x = torch.tanh(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","\n","# Create the ANN model\n","ann = ANN()\n","\n","# Define the loss function and optimizer\n","loss_fn = nn.MSELoss()\n","optimizer = optim.Adam(ann.parameters(), lr=0.01)\n","\n","# Convert the data to PyTorch tensors\n","x_data = torch.tensor(x_data.reshape(-1, 1), dtype=torch.float32)\n","y_data = torch.tensor(y_data.reshape(-1, 1), dtype=torch.float32)\n","x_train = torch.tensor(x_train.reshape(-1, 1), dtype=torch.float32)\n","y_train = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n","x_test = torch.tensor(x_test.reshape(-1, 1), dtype=torch.float32)\n","y_test = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n","\n","\n","val_plot=[]\n","loss_plot=[]\n","# Train the ANN model\n","for epoch in range(1000):\n","    # Forward pass\n","    y_pred = ann(x_train)\n","    y_val = ann(x_test)\n","\n","    # Compute the loss\n","    loss = loss_fn(y_pred, y_train)\n","    val = loss_fn(y_val, y_test)\n","    val_plot.append(val.item())\n","    loss_plot.append(loss.item())\n","\n","    # Print the loss every 100 epochs\n","    if (epoch+1) % 100 == 0:\n","      print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Val: {loss.item():.4f}')\n","\n","\n","    # Backpropagation\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","# Convert the sample data to a PyTorch tensor\n","x_sample = torch.tensor(x_sample.reshape(-1, 1), dtype=torch.float32)\n","#x_sample = x_sample.reshape(-1, 1).clone().detach()\n","\n","\n","# Predict the output for the sample data\n","y_pred_samples = ann(x_sample)\n","\n","# Plot the results\n","figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n","ax1.plot(y_train, y_train, color='red', label='Test set')\n","ax1.plot(y_train, y_pred.detach().numpy(), \"o\", ms=6, color='blue', alpha=0.5, label='Predicted by the model')\n","\n","ax1.set_title('ANN, N_train = 160, N_val = 40', fontsize=14)\n","ax1.set_xlabel('y (test)', fontsize=16)\n","ax1.set_ylabel('y (predict)', fontsize=16)\n","ax1.legend(fontsize=12)\n","\n","\n","ax2.plot(x_data, y_data, 'o', ms=4, color='red', alpha=0.3, label='Full Dataset')\n","ax2.plot(x_train, y_train, 'o', ms=6, color='blue', alpha=1, label='Train Data')\n","ax2.plot(x_sample, y_pred_samples.detach().numpy(), color='orange', label='prediction of ANN')\n","\n","ax2.set_xlabel('x', fontsize=16)\n","ax2.set_ylabel('y', fontsize=16)\n","ax2.set_ylim(ymin=-2, ymax=2)\n","ax2.legend(fontsize=10)\n","\n","plt.show()\n","\n","################################################\n","#\n","fig, ax = plt.subplots(figsize=(12, 6))\n","ax.set_title(\"Multi-layer Perceptron\")\n","ax.set_xlabel(\"Iterations\")\n","ax.set_ylabel(\"Loss\")\n","ax.set_xscale('log')\n","ax.set_yscale('log')\n","\n","ax.plot(loss_plot,label=\"Train set\")\n","ax.plot(val_plot,label=\"Validation set\")\n","ax.legend()\n","plt.show()"],"metadata":{"id":"4QjZgH4fCSD6"},"execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"collapsed_sections":["WuPs0ogyiT7L","aOdozo77iT7P","isoqNFW01xs9","vAxRjpRW166i","J5PCc9YD2TRH","_9wRyef1iT7U","76VIYELM15pR"]}},"nbformat":4,"nbformat_minor":0}