{"cells":[{"cell_type":"markdown","id":"3f357242-9dd7-4df5-a3f9-004989b233da","metadata":{"id":"3f357242-9dd7-4df5-a3f9-004989b233da"},"source":["# Hands-on 1: Clustering"]},{"cell_type":"markdown","id":"1d426fd4-d9aa-4cc7-8b9c-4d24500a0226","metadata":{"id":"1d426fd4-d9aa-4cc7-8b9c-4d24500a0226"},"source":["Clustering technics are a particular class of unsupervised Machine Learning, whose objective is to separate your data into homogeneous groups with common characteristics. With those, one can find groups of similar objects, objects that are more related to each other than to objects in other groups. Examples of business-oriented applications of clustering include the grouping of music, news, movies, or even people, finding customers that share similar interests based on common behaviors/interests as a basis for recommendation engines.\n","\n","The following four types are the most widely used types of clustering models.\n","\n","* **Centroid Models**: uses the distance between a data point and the centroid of the cluster to group data. K-means clustering is an example of a centroid model.\n","* **Distribution Models**: segments data based on their probability of belonging to the same distribution. Gaussian Mixture Model (GMM) is a popular distribution model.\n","* **Connectivity Models**: uses the closeness of the data points to decide the clusters. Hierarchical Clustering Model is a widely used connectivity model.\n","* **Density Models**: scans the data space and assigns clusters based on the density of data points. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular density model.\n","\n","[More Information here](https://scikit-learn.org/stable/modules/clustering.html).\n","\n"]},{"cell_type":"markdown","id":"f710b35e-de05-49a0-ae89-a1797fe5cb93","metadata":{"id":"f710b35e-de05-49a0-ae89-a1797fe5cb93"},"source":["\n","\n","---\n","\n","\n","## K-means"]},{"cell_type":"markdown","id":"71df1e78-22cf-405f-ba63-147b72221d0b","metadata":{"id":"71df1e78-22cf-405f-ba63-147b72221d0b"},"source":["The K-means algorithm is a very well known unsupervised algorithm in clustering. In this lab practice we will detail how it works and the useful ways to optimize it.\n","\n","The basic question is how do we measure similarity between objects? We can define similarity as the opposite of diference, and a commonly used metric for diference between exemples for clustering samples with continuous features is the squared Euclidean distance between two points x and y in m-dimensional space:\n","$$\n","d^2(\\mathbf{x},\\mathbf{y}) = \\sum_{j=1}^m(x_j-y_j)^2 = \\Vert \\mathbf{x}-\\mathbf{y}\\Vert^2_2  \n","$$\n","\n","Based on this Euclidean distance metric, we can describe the k-means algorithm (one of the most basic clustering technics) as a optimization problem for minimizing the within-cluster Sum of Squared Errors (SSE, also called cluster inertia or distortion), defined as follows:\n","\n","$$\n","\\mathrm{SSE} = \\sum_{i=1}^n \\sum_{j=1}^k w^{(i,j)} d^2(\\mathbf{x}^{(i)},\\mu^{(j)})\n","$$\n","\n","where $n$ is the number of samples, $k$ is the number of clusters, $\\mu^{(j)}$ is the centroid of the $j$-th cluster, $\\mathbf{x}^{(i)}$ is on the $j$-th cluster if $\\mu^{(j)}$ is tha closest centroid to it, and $w^{(i,j)}=1$ if $\\mathbf{x}^{(i)}$ is in cluster $j$ and $0$ otherwise.\n","\n","*For information:* This algorithm was designed in 1957 at Bell Laboratories by Stuart P. Lloyd as a pulse code modulation (PCM) technique. It was presented to the general public only in 1982. In 1965 Edward W. Forgy had already published an almost similar algorithm, which is why K-means is often called the Lloyd-Forgy algorithm. The fields of application are diverse: customer segmentation, data analysis, image segmentation, semi-supervised learning etc."]},{"cell_type":"markdown","id":"8ce20e92-a407-4323-b4e4-5dea6976e69f","metadata":{"id":"8ce20e92-a407-4323-b4e4-5dea6976e69f"},"source":["### The principle of the k-means algorithm\n","\n","Given a set of points and an integer $k$, the algorithm aims to divide the points into $k$ homogeneous and compact groups, called clusters. Let's look at the example below:"]},{"cell_type":"code","execution_count":null,"id":"6d3096dd-200e-40c4-929d-37bee81ecc48","metadata":{"id":"6d3096dd-200e-40c4-929d-37bee81ecc48"},"outputs":[],"source":["#Just the basics\n","import numpy as np\n","import pandas as pd\n","\n","# Plotting te things\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","from matplotlib import colors\n","mpl.rc('image', cmap='jet')\n","\n","# Mathematical Analysis\n","from scipy import linalg\n","from scipy.spatial import Voronoi, voronoi_plot_2d\n","\n","# Metrics\n","import sklearn.metrics as metrics\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from sklearn.neighbors import NearestCentroid\n","\n","# Dataset\n","from sklearn import datasets\n","from sklearn.datasets import make_blobs\n","\n","# Dimensionality reduction\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","# Modeling\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","from sklearn.cluster import DBSCAN\n"]},{"cell_type":"markdown","id":"59974eb7-efd9-4a66-bc89-8b7f4448bcca","metadata":{"id":"59974eb7-efd9-4a66-bc89-8b7f4448bcca"},"source":["1. Let's create a dataset of size $N=2000$, distributed in three clusters with a Gaussian distribution,"]},{"cell_type":"code","execution_count":null,"id":"80aaf506-af54-458f-bfdd-045dc4359e2b","metadata":{"id":"80aaf506-af54-458f-bfdd-045dc4359e2b"},"outputs":[],"source":["n_samples = 2000\n","random_state = 130 # fix the random state for reproducibility\n","n_components=3\n","std_dev=1.0\n","# This function makes some clusters with 2D coordinates in X and a label y\n","# Usefull for testing unsupervised ML as well classification\n","\n","X,y = make_blobs(n_samples=n_samples, centers=n_components, cluster_std=std_dev, random_state=random_state)\n","\n","figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","ax1.scatter(X[:,0], X[:,1])\n","ax1.set_title ('Clusters',fontsize=20)\n","ax2.scatter(X[:,0], X[:,1], c = y)\n","ax2.set_title ('Classes',fontsize=20)\n","xlim, ylim = ax1.get_xlim(),ax1.get_ylim()"]},{"cell_type":"markdown","id":"5e87a1cc-5397-4c13-8cf4-a83956222762","metadata":{"id":"5e87a1cc-5397-4c13-8cf4-a83956222762"},"source":["2. Create a model with $k=3$. As we set `n_components=3` while generating the data, this should be a good guess on how many clusters are in our data."]},{"cell_type":"code","execution_count":null,"id":"362cd40f-590f-4cf6-a949-ffd5ddbbecb4","metadata":{"id":"362cd40f-590f-4cf6-a949-ffd5ddbbecb4"},"outputs":[],"source":["# Fix the number of clusters that you want\n","nb_c = 3\n","\n","# create a K-means model with that number of clusters\n","model = KMeans(n_clusters=nb_c\n","               , init='random'\n","               , n_init=10\n","               , max_iter=300\n","               , tol=1e-04\n","               , random_state=random_state\n","              )\n","\n","# cluster the data with that model\n","model.fit(X)\n","\n","# here for the demonstration use the same data to see how it performs\n","# y_pred :labels assigned to each data points\n","# centers : centroid positions of each cluster\n","\n","y_pred = model.predict(X)\n","centers = model.cluster_centers_\n","# Plot the data\n","\n","vor = Voronoi(centers)\n","voronoi_plot_2d(vor\n","                , show_vertices=False\n","                , line_colors='red'\n","                , line_width=2\n","                , line_alpha=0.6\n","                , point_size=10)\n","\n","plt.scatter(X[:, 0], X[:,1], c = y_pred)\n","plt.xlim(xlim)\n","plt.ylim(ylim)\n","plt.title ('Clusters',fontsize=20)\n","plt.show()"]},{"cell_type":"markdown","id":"ebdbaa54-4494-4547-9ae9-da2966707fe7","metadata":{"id":"ebdbaa54-4494-4547-9ae9-da2966707fe7"},"source":["**Note**: The cluster labels can be different but this is not important, so seems to perform perfectly as can be seen from the initial 3 classes that have been built.\n","\n","k-means divides the space using hyperplanes to attibute a data point to a cluster. For 2D data, these planes can visualized by a Vorono√Ø analysis. This will be more clear when increasing the requested number of clusters bellow and can help predicting to which cluster a new data point should be attributed."]},{"cell_type":"markdown","id":"c991cb42-850f-4835-a458-4a203302970d","metadata":{"id":"c991cb42-850f-4835-a458-4a203302970d"},"source":["3. What if we change the wanted number of clusters ?"]},{"cell_type":"code","execution_count":null,"id":"34837839-9ec2-46b0-8002-edfeaf762582","metadata":{"id":"34837839-9ec2-46b0-8002-edfeaf762582"},"outputs":[],"source":["# Fix the number of clusters that you want\n","y_pred = np.zeros(n_samples) #create an empty array\n","random_state = 1234\n","k_max = 10\n","nr = k_max//2\n","\n","models = []\n","\n","figure, ax = plt.subplots(nrows=nr, ncols = 2, figsize=(14,5*nr))\n","\n","for nb_c in range(1,k_max+1):\n","\n","# create a K-means model with that number of clusters\n","    model = KMeans(n_clusters=nb_c\n","               , init='random'\n","               , n_init=10\n","               , max_iter=300\n","               , tol=1e-04\n","               , random_state=random_state\n","              )\n","# cluster the data with that model\n","    model.fit(X)\n","\n","# here for thedemonstration use the same data to see how it performs\n","    y_pred = np.append(y_pred, model.predict(X))\n","    models.append(model)\n","\n","y_p = np.reshape(y_pred,(k_max+1,n_samples))\n","\n","for nb_c in range(1,k_max+1):\n","    ax[(nb_c-1)//2,1-nb_c%2].scatter(X[:, 0], X[:,1], c = y_p[nb_c,:])\n","    ax[(nb_c-1)//2,1-nb_c%2].set_title(f'number of clusters: {nb_c}', fontsize = 20)\n","    ax[(nb_c-1)//2,1-nb_c%2].scatter(models[nb_c-1].cluster_centers_[:,0], models[nb_c-1].cluster_centers_[:,1],\n","                marker='*',\n","                color='red',\n","                s=200);\n","\n","    if nb_c>2:\n","        vor = Voronoi(models[nb_c-1].cluster_centers_)\n","        voronoi_plot_2d(vor, ax[(nb_c-1)//2,1-nb_c%2]\n","            , show_vertices=False\n","            , line_colors='red'\n","            , line_width=2\n","            , line_alpha=0.6\n","            , point_size=10\n","            , point_alpha=0\n","            )\n","    ax[(nb_c-1)//2,1-nb_c%2].set_xlim(xlim)\n","    ax[(nb_c-1)//2,1-nb_c%2].set_ylim(ylim)\n"]},{"cell_type":"markdown","id":"786fdf55-9dec-4c88-b244-d42175866d61","metadata":{"id":"786fdf55-9dec-4c88-b244-d42175866d61"},"source":["One can say that the partitionning is not ideal because the number of initial clusters is either lower or higher than the visual impression (except for $k=3$, the we know by construction as the real number of clusters). The fact is that in real data, we usualy dont know the number of clusters and maybe this number is not unique."]},{"cell_type":"markdown","id":"5c027c9d-08d5-4f74-a428-e54421b868eb","metadata":{"id":"5c027c9d-08d5-4f74-a428-e54421b868eb"},"source":["3. Search for the optimal number of clusters\n","\n","If you dont have any intuition about your data or want to be sure while choosing, there are methods to help you determine the ideal number of clusters.\n","\n","####  Elbow method\n","This method is based on the notion of inertia, the same one we defined above as SSE. For k-means, the inertia monotonicaly decreases with number of clusters (the inertia is reduced with $k$ cause the points will be next to their centroids). One can choose the value of $k$ as the one for what there is no more significant reduction on the inertia if the number of clusters in in creased.\n","\n","Let's look at what this gives on our example:"]},{"cell_type":"code","execution_count":null,"id":"b02d75c4-eb6d-41d7-af99-cc8e1aaac958","metadata":{"id":"b02d75c4-eb6d-41d7-af99-cc8e1aaac958"},"outputs":[],"source":["# Extract the inertia\n","\n","elb = []\n","for i in range(k_max):\n","    elb.append(models[i].inertia_)\n","\n","\n","plt.plot(range(1,k_max+1),elb, marker = 'o')\n","plt.xticks(range(1,k_max+1))\n","plt.xlabel('Number of clusters in k-means', fontsize=16)\n","plt.ylabel('Inertia or Distortion', fontsize=16)\n","plt.show()"]},{"cell_type":"markdown","id":"47682ce1-5d00-4664-8432-b41c7cb29221","metadata":{"id":"47682ce1-5d00-4664-8432-b41c7cb29221"},"source":["One can notice that the inertia stagnates after 3 clusters. This method is conclusive here. Nevertheless it can be coupled with a more precise approach but which requires more computing time.\n","\n","#### Silhouette coefficient\n","\n","The silhouette coefficient is defined as\n","\n","$$\n","s(\\mathbf{x}^{(i)}) = \\frac{b-a}{\\max(a,b)},\n","$$\n","\n","where $a$ is the average of the distances between the sample $\\mathbf{x}^{(i)}$ to all other samples inside the same cluster (i.e., the intra-cluster average), and $b$ is the average distance from $\\mathbf{x}^{(i)}$ to the samples in the nearest cluster. This coefficient can vary between $-1$ and $+1$. A $s$ coefficient close to $+1$ means that the observation is well located inside its own cluster, while a coefficient close to $0$ means that it is located near a border; finally, a coefficient close to $-1$ means that the observation is associated with the wrong cluster. To analyze the full set, the avarage $<s(\\mathbf{x}^{(i)})>_i$ should be considered.\n","\n","The calculation of this coefficient is included in the `sklearn.metrics` library. As for the inertia, lets display the evolution of the coefficient as a function of the number of clusters for our example:"]},{"cell_type":"code","execution_count":null,"id":"8667335a-3814-4338-82a3-dfaa2259854c","metadata":{"id":"8667335a-3814-4338-82a3-dfaa2259854c"},"outputs":[],"source":["\n","# Extract the silhouette\n","\n","sil = []\n","for i in range(2,k_max+1):\n","    sil.append(silhouette_score(X, y_p[i]))\n","\n","    print(\n","        \"For n_clusters =\",\n","        i,\n","        \"The average silhouette_score is :\",\n","        sil[-1],\n","    )\n","plt.plot(range(2,k_max+1),sil, marker = 'o')\n","plt.xticks(range(2,k_max+1))\n","plt.xlabel('Number of clusters in k-means', fontsize=16)\n","plt.ylabel('Silhouette coefficient', fontsize=16)\n","plt.show()"]},{"cell_type":"markdown","id":"ab87a628-922d-4d84-aa72-f314e9318086","metadata":{"id":"ab87a628-922d-4d84-aa72-f314e9318086"},"source":["4. Predict now to what cluster belong new points according to the best model\n","\n","It is usefull to use the voronoi to visually check the performance of the clustering and see how it works"]},{"cell_type":"code","execution_count":null,"id":"d3a2722b-048b-4525-b98b-1b4bcf052cfd","metadata":{"id":"d3a2722b-048b-4525-b98b-1b4bcf052cfd"},"outputs":[],"source":["# Select dim points randomly\n","n = 100\n","dim = 1\n","np.random.seed(42)\n","\n","x = np.array([])\n","x = np.append(x,np.random.rand(n, dim)*14-10)\n","x = np.append(x,np.random.rand(n, dim)*22-11)\n","X_new = np.reshape(x,(2,n)).transpose()\n","\n","# Best model\n","nb_c = 3\n","\n","y_p_new = models[nb_c-1].predict(X_new)\n","centers = models[nb_c-1].cluster_centers_\n","\n","figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","\n","vor = Voronoi(centers)\n","\n","ax1.scatter(X[:,0],X[:,1], c = y_p[nb_c])\n","voronoi_plot_2d(vor\n","                 , ax1\n","                 , show_vertices=False\n","                 , line_colors='red'\n","                 , line_width=2\n","                 , line_alpha=0.6\n","                 , point_size=10)\n","\n","ax2.scatter(X_new[:,0],X_new[:,1], c = y_p_new)\n","voronoi_plot_2d(vor\n","                 , ax2\n","                 , show_vertices=False\n","                 , line_colors='red'\n","                 , line_width=2\n","                 , line_alpha=0.6\n","                 , point_size=10)\n","ax1.set_xlim(xlim)\n","ax1.set_ylim(ylim)\n","ax2.set_xlim(xlim)\n","ax2.set_ylim(ylim)\n","plt.show()"]},{"cell_type":"markdown","id":"21510d68-561e-48b6-b76c-a647800fb625","metadata":{"id":"21510d68-561e-48b6-b76c-a647800fb625"},"source":["#### Practice this on more tricky example"]},{"cell_type":"code","execution_count":null,"id":"204e41c3-9793-4567-b4d3-72fb0c43b92f","metadata":{"id":"204e41c3-9793-4567-b4d3-72fb0c43b92f"},"outputs":[],"source":["n_samples = 2000\n","random_state = 0 # fix the random state for reproducibility\n","n_components = 4\n","std_dev=1\n","# This function makes some clusters with 2D coordinates in X and a label y\n","# Usefull for testing unsupervised ML as well classification\n","\n","X,y = make_blobs(n_samples=n_samples, centers=n_components, cluster_std=std_dev, random_state=random_state)\n","\n","figure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n","\n","ax1.scatter(X[:,0], X[:,1])\n","ax1.set_title ('Clusters',fontsize=20)\n","ax2.scatter(X[:,0], X[:,1], c = y)\n","ax2.set_title ('Classes',fontsize=20)\n","xlim, ylim = ax1.get_xlim(),ax1.get_ylim()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"cbe355c0-ee75-4e23-ae15-4a0dc74b901c","metadata":{"id":"cbe355c0-ee75-4e23-ae15-4a0dc74b901c"},"outputs":[],"source":["# create a K-means model with that number of clusters\n","(...)\n","\n","# cluster the data with that model\n","(...)\n","\n","# As demonstrated above, use the same data for diverent k to see how it performs\n","(...)\n"]},{"cell_type":"code","execution_count":null,"id":"9854fbf3-ba81-4e16-87fb-f4cdd42c7dc3","metadata":{"id":"9854fbf3-ba81-4e16-87fb-f4cdd42c7dc3"},"outputs":[],"source":["# Extract and plot the inertia\n","(...)\n","\n","\n","# Extract and plot the silhouette\n","(...)\n"]},{"cell_type":"markdown","id":"c921f783-4b47-46ae-bf8a-75ef757ec39e","metadata":{"id":"c921f783-4b47-46ae-bf8a-75ef757ec39e"},"source":["### Conclusion:\n","\n","Here k-means still perdorms quite well, even if the inertia and siloutte score are less clear.\n","\n","Some final remarks about k-means that where not adressed here:\n","\n","* Advantages: k-Means is fast and scalable\n","\n","* Drawbacks: The model performance is highly impacted by the initial centroids. Some centroids initiation can produce sub-optimal results. k-Means model does not perform well when the cluster sizes vary a lot, have different densities, or have a non-spherical shape.\n","\n","* Extentions: k-means++ which contains a more clever way to initialize the centroids, often used in the Gaussian Mixture Model.  "]},{"cell_type":"markdown","id":"67b6c9f8-170d-4f13-a065-7dc7658b6827","metadata":{"id":"67b6c9f8-170d-4f13-a065-7dc7658b6827"},"source":["\n","\n","---\n","\n","\n","## Gaussian Mixture Model"]},{"cell_type":"markdown","id":"f073bfad-7b7e-4676-8160-c41ad1cf6a9b","metadata":{"id":"f073bfad-7b7e-4676-8160-c41ad1cf6a9b"},"source":["Gaussian Mixture Model (GMM) is a probabilistic model that assumes each data point belongs to a Gaussian distribution. It uses the expectation-maximization (EM) algorithm.\n","\n","In the expectation step, the algorithm estimates the probability of each data point belonging to each cluster.\n","In the maximization step, each cluster is updated using the estimated probability of belonging to the cluster of all the data points.\n","The updates of the cluster are mostly impacted by the data points with high probabilities of belonging to the cluster.\n","\n","Since we are using the already done `sklearn` Python routines, the code here for GMM is similar to the one for the k-means, we just need to change the method from `KMeans` to `GaussianMixture`.\n","\n","One difference is that the value for n_init from the default value of $1$ is changed to $5$. n_init is the number of initializations to generate. When setting it to $5$, it means that $5$ initializations for the model will be performed, and the one with the best result is kept."]},{"cell_type":"code","execution_count":null,"id":"b3803440-f5cd-4ff5-9cad-40cc005076b5","metadata":{"id":"b3803440-f5cd-4ff5-9cad-40cc005076b5"},"outputs":[],"source":["k_max = 10\n","nr = k_max//2\n","\n","models_gmm = []\n","y_pred = np.zeros(n_samples) #create an empty array\n","\n","figure, ax = plt.subplots(nrows=nr, ncols = 2, figsize=(14,5*nr))\n","\n","for nb_c in range(1,k_max+1):\n","\n","# create a GMM model with that number of clusters\n","    model = GaussianMixture(n_components=nb_c\n","                            , n_init= 5\n","                            , random_state=random_state)\n","\n","# cluster the data with that model\n","    model.fit(X)\n","\n","# here for the¬†demonstration use the same data to see how it performs\n","    y_pred= np.append(y_pred, model.predict(X))\n","    models_gmm.append(model)\n","\n","y_p_gmm  = np.reshape(y_pred,(k_max+1,n_samples))\n","\n","for nb_c in range(1,k_max+1):\n","    ax[(nb_c-1)//2,1-nb_c%2].scatter(X[:, 0], X[:,1], c = y_p_gmm[nb_c,:])\n","    ax[(nb_c-1)//2,1-nb_c%2].set_title(f'number of clusters: {nb_c}', fontsize = 20)\n","    ax[(nb_c-1)//2,1-nb_c%2].scatter(models_gmm[nb_c-1].means_[:,0], models_gmm[nb_c-1].means_[:,1],\n","                marker='*',\n","                color='red',\n","                s=200);\n"]},{"cell_type":"code","execution_count":null,"id":"67d9fd67-c561-4e96-96e8-3468573d5281","metadata":{"id":"67d9fd67-c561-4e96-96e8-3468573d5281"},"outputs":[],"source":["# Since sklearn do not give a standard inertia output for GMM, here we extract the log-likelyhood, AIC and BIC.\n","# Check https://scikit-learn.org/stable/modules/linear_model.html#aic-bic for the mathematical definitions and the meaning of each metric.\n","\n","elb = []\n","aic = []\n","bic = []\n","for i in range(k_max):\n","    elb.append(models_gmm[i].score(X))\n","    aic.append(models_gmm[i].aic(X))\n","    bic.append(models_gmm[i].bic(X))\n","\n","# Extract the silhouette\n","sil = []\n","for i in range(2,k_max+1):\n","    sil.append(silhouette_score(X, y_p_gmm[i]))\n","\n","    print(\n","        \"For n_clusters =\",\n","        i,\n","        \"The average silhouette_score is :\",\n","        sil[-1],\n","    )\n","\n","figure, axs = plt.subplots(nrows = 2, ncols=2, figsize=(14,10))\n","\n","axs[0,0].plot(range(1,k_max+1),elb, marker = 'o')\n","axs[0,0].set_xticks(range(1,k_max+1))\n","axs[0,0].set_xlabel('Number of clusters in gmm', fontsize=16)\n","axs[0,0].set_ylabel('Log Likelyhood ', fontsize=16)\n","\n","axs[0,1].plot(range(2,k_max+1),sil, marker = 'o')\n","axs[0,1].set_xticks(range(2,k_max+1))\n","axs[0,1].set_xlabel('Number of clusters in gmm', fontsize=16)\n","axs[0,1].set_ylabel('Silhouette coefficient', fontsize=16)\n","\n","axs[1,0].plot(range(1,k_max+1),aic, marker = 'o')\n","axs[1,0].set_xticks(range(1,k_max+1))\n","axs[1,0].set_xlabel('Number of clusters in gmm', fontsize=16)\n","axs[1,0].set_ylabel('Akaike information Criterion', fontsize=16)\n","\n","axs[1,1].plot(range(1,k_max+1),bic, marker = 'o')\n","axs[1,1].set_xticks(range(1,k_max+1))\n","axs[1,1].set_xlabel('Number of clusters in gmm', fontsize=16)\n","axs[1,1].set_ylabel('Bayesian Information Criterion', fontsize=16)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"faa846d5-fa75-4150-9140-7fbf3f16ca1e","metadata":{"id":"faa846d5-fa75-4150-9140-7fbf3f16ca1e"},"source":["\n","\n","---\n","\n","\n","## Hierachical clustering"]},{"cell_type":"markdown","id":"8ff5e8a1-383b-438a-a3f0-66cb502ddd75","metadata":{"id":"8ff5e8a1-383b-438a-a3f0-66cb502ddd75"},"source":["Hierarchical clustering analysis (HCA) is a umbrela term for a family of clustering methods that are used to build a hierarchy of clusters. The hierarchical clustering algorithms produce a binary tree where the root of the tree includes all the data points, and the leaves of the tree are the individual data points. Their are divided in *agglomerative* (bottom-up, i.e., we start with all samples in 1-sampled clusters and merge then to go up in the hyerarchy) and *divisive* (top-down, i.e., all samples start in the same cluster, where splits mean moving down in the hierarchy).\n","\n","Here we will use the `AgglomerativeClustering` from `sklearn`, that uses the bottom-up approach. Manny strategies can be used to merge the clusters (cansulte the ones available in this function [here](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)), but here we will use the default `Ward` approach, that searchs to minimize variance inside the clusters."]},{"cell_type":"code","execution_count":null,"id":"93f460ae-204f-45ed-8b9e-86e52b59d363","metadata":{"id":"93f460ae-204f-45ed-8b9e-86e52b59d363"},"outputs":[],"source":["k_max = 10\n","nr = k_max//2\n","\n","models_hc = []\n","y_pred = np.zeros(n_samples) #create an empty array\n","\n","figure, ax = plt.subplots(nrows=nr, ncols = 2, figsize=(14,5*nr))\n","\n","for nb_c in range(1,k_max+1):\n","\n","# create a GMM model with that number of clusters\n","    model = AgglomerativeClustering(n_clusters=nb_c)\n","\n","# cluster the data with that model\n","    model.fit_predict(X)\n","\n","# here for the¬†demonstration use the same data to see how it performs\n","    y_pred= np.append(y_pred, model.fit_predict(X))\n","    models_hc.append(model)\n","\n","y_p_hc  = np.reshape(y_pred,(k_max+1,n_samples))\n","\n","\n","# for this method calculate the centroids externally\n","clf = NearestCentroid()\n","\n","for nb_c in range(1,k_max+1):\n","    ax[(nb_c-1)//2,1-nb_c%2].scatter(X[:, 0], X[:,1], c = y_p_hc[nb_c,:])\n","    ax[(nb_c-1)//2,1-nb_c%2].set_title(f'number of clusters: {nb_c}', fontsize = 20)\n","    if nb_c>1:\n","        clf.fit(X, y_p_hc[nb_c])\n","        ax[(nb_c-1)//2,1-nb_c%2].scatter(clf.centroids_[:,0], clf.centroids_[:,1],\n","                marker='*',\n","                color='red',\n","                s=200);\n"]},{"cell_type":"code","execution_count":null,"id":"8906ff1b-a11f-4543-8aa5-94336430a882","metadata":{"tags":[],"id":"8906ff1b-a11f-4543-8aa5-94336430a882"},"outputs":[],"source":["# Extract the silhouette\n","sil = []\n","for i in range(2,k_max+1):\n","    sil.append(silhouette_score(X, y_p_hc[i]))\n","\n","    print(\n","        \"For n_clusters =\",\n","        i,\n","        \"The average silhouette_score is :\",\n","        sil[-1],\n","    )\n","\n","figure, (ax) = plt.subplots(nrows=1, ncols=1, figsize=(7,5))\n","\n","ax.plot(range(2,k_max+1),sil, marker = 'o')\n","ax.set_xticks(range(2,k_max+1))\n","ax.set_xlabel('Number of clusters in k-means', fontsize=16)\n","ax.set_ylabel('Silhouette coefficient', fontsize=16)\n","\n","\n","\n","plt.show()"]},{"cell_type":"markdown","id":"19a9f74b-673f-4e8e-8918-a982a2794cad","metadata":{"id":"19a9f74b-673f-4e8e-8918-a982a2794cad"},"source":["\n","---\n","\n","## Density-based spatial clustering of applications with noise  (DBSCAN)\n","\n","\n"]},{"cell_type":"markdown","id":"5ab78cee-17a2-49d7-b77d-ac7f0581c7ac","metadata":{"id":"5ab78cee-17a2-49d7-b77d-ac7f0581c7ac"},"source":["DBSCAN defines clusters using data density. It has two important hyperparameters to tune, `eps` and `min_samples`.\n","\n","* `eps` is the epsilon distance to be considered as the neighborhood of a data point. It is the most important parameter for DBSCAN.\n","* `min_samples` is the number of minimum data points in the neighborhood  of a point (including itself) in order for that point to be defined as a *core data point*.\n","\n","All data points in the neighborhood of the core data points belong to the same cluster, what allows to groupings of core points to be formed in the same cluster. The data points that do not have a core data point in the neighborhood are considered outliers and are marked in the output of this method with the label `-1`.\n","\n","**An important distinction with respect to the other methods is that DBSCAN does not take a pre-defined number of clusters**, it identifies the number of clusters based on the density distribution of the dataset."]},{"cell_type":"code","execution_count":null,"id":"29de1a79-d144-480e-a972-c004ef559497","metadata":{"id":"29de1a79-d144-480e-a972-c004ef559497"},"outputs":[],"source":["# First go back to the first sample\n","\n","X0,y0 = make_blobs(n_samples=n_samples, centers=3, cluster_std=1.0, random_state=130)\n","model_db0 = DBSCAN(eps = 0.6, min_samples = 3) # <<------------ Try playing with those values and see how it changes the result\n","y_p_db0= model_db0.fit_predict(X0)\n","\n","\n","# Then the more tricky sample\n","model_db = DBSCAN(eps = 0.6, min_samples = 3) # <<------------ Try playing with those values and see how it changes the result\n","\n","# cluster the data with that model\n","y_p_db= model_db.fit_predict(X)\n","\n","print('Simple sample: ')\n","labels0 = model_db0.labels_\n","n_clusters0 = len(set(labels0)) - (1 if -1 in labels0 else 0)\n","n_noise0 = list(labels0).count(-1)\n","print(\"Estimated number of clusters: %d\" % n_clusters0)\n","print(\"Estimated number of noise points: %d\" % n_noise0)\n","print('\\n')\n","print('Tricky sample: ')\n","labels = model_db.labels_\n","n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","n_noise = list(labels).count(-1)\n","\n","print(\"Estimated number of clusters: %d\" % n_clusters)\n","print(\"Estimated number of noise points: %d\" % n_noise)\n","\n","figure, (ax1,ax2) = plt.subplots(nrows=1, ncols = 2, figsize=(14,5))\n","\n","ax1.scatter(X0[:, 0], X0[:,1], c = y_p_db0)\n","ax1.set_title(f'Estimated number of clusters: {n_clusters0}', fontsize = 20)\n","ax2.scatter(X[:, 0], X[:,1], c = y_p_db)\n","ax2.set_title(f'Estimated number of clusters: {n_clusters}', fontsize = 20)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"34971bcc-a0ba-4e77-90fb-d191928b5ec6","metadata":{"id":"34971bcc-a0ba-4e77-90fb-d191928b5ec6"},"source":["For the first simple sample, DBSCAN identifies the correct number of clusters, at the condition that two parameters are correctly set. In that case the tuning is somewhat easy. Here the number of cluster is 4 out of which 1 has a label -1 corresponding to the noise. The latter is tipicaly to be removed, but beware thhat it is very susseptible to the balance between noise and the chosen density of the neighborhood.\n","\n","For the more tricky sample we can see that DBSCAN has some difficulty at first sight to identify the clusters that overlap, and a more deeper tuning should be done (for instance, try `DBSCAN(eps = 0.4, min_samples = 20)`).  \n","\n","DBSCAN Pros: works on datasets of any shape (the clusters can be not gaussian, they just need to be connected by core data points) and identifies anomalies automatically. The automatic selection of number of clusters can also be a plus in unknown data.\n","\n","DBSCAN Cons: It does not work well for identifying the clusters that are not well separated (as almost all clustering methods). Different clusters in the dataset need to have similar densities, otherwise, the DBSCAN does not perform well.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[],"collapsed_sections":["f710b35e-de05-49a0-ae89-a1797fe5cb93","67b6c9f8-170d-4f13-a065-7dc7658b6827","faa846d5-fa75-4150-9140-7fbf3f16ca1e","19a9f74b-673f-4e8e-8918-a982a2794cad"]}},"nbformat":4,"nbformat_minor":5}